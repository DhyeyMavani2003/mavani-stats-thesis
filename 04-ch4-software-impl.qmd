# Software (Package) Implementation and Testing {#sec-software}

```{r}
#| label: setup
#| include: false

# Load knitr package
library(knitr)

# Load packages
library(tidyverse)
library(gt)

# Set default ggplot theme for document
theme_set(theme_classic())
# If using kableExtra tables, print blank cells instead of `NA`
options(knitr.kable.NA = "")
```

In this chapter, we introduce `ccrvam`, a Python package that implements the Checkerboard Copula Regression-based Visualization and Association Measure (CCRVAM) techniques discussed in previous chapters. Despite the growing importance of multivariate categorical data analysis with ordinal response variables in disciplines like medicine, social sciences, and economics, there has been a notable lack of user-friendly, well-tested software implementations that scale efficiently to higher-dimensional problems. The `ccrvam` package addresses this gap by providing a comprehensive suite of tools for analyzing multivariate discrete data using the checkerboard copula approach.

## Set-up and Example Data

The `ccrvam` package is built for analyzing multi-dimensional contingency tables with an ordinal response variable and a set of categorical (nominal/ordinal) explanatory variables/predictors. This aligns perfectly with the theoretical frameworks established in @sec-ccrvam. The package is designed with ease of installation and use in mind, particularly within Jupyter Notebook environments common in data analysis workflows.

### Installation

For quick use in Jupyter Notebooks, the package can be installed directly from PyPI with:

```{bash}
#| label: install-bash
#| eval: true
#| echo: true
#| output: false

pip install ccrvam==0.9.6
```

For more containerized production-heavy work, it's recommended to use a custom virtual environment. Instructions for setting that up can be found in [ccrvam/.github/README.md](https://github.com/DhyeyMavani2003/ccrvam?tab=readme-ov-file#readme).

## Types of Input Data Supported

The `ccrvam` package is designed to be flexible with respect to input data formats. The package supports these as main formats:

### Loading from Contingency Table Format (In-Place)

The `ccrvam` package implements the theoretical framework outlined in @sec-ccrvam by operating on multi-dimensional contingency tables. These tables represent the joint distribution of categorical variables where: one variable is explicitly designated as an ordinal response (dependent variable); one or more variables function as categorical predictors (independent variables); and table entries contain frequency counts of observations. The package accepts these contingency tables directly as NumPy arrays, with each dimension corresponding to the categories of a particular variable. This structure allows for efficient computation of the checkerboard copula scores and subsequent association measures while preserving the natural ordering of the response variable categories and accommodating multiple categorical predictors simultaneously.

For example, consider the 2-D example from the previous chapter:

```{python}
#| label: example-init
#| echo: true
#| output: true

import numpy as np
from ccrvam import GenericCCRVAM

# Migraine treatment example (dose vs. pain severity)
contingency_table_2d = np.array([
    [0, 0, 20],
    [0, 10, 0],
    [20, 0, 0],
    [0, 10, 0],
    [0, 0, 20]
])

# Create a CCRVAM object
ccrvam_obj = GenericCCRVAM.from_contingency_table(contingency_table_2d)
# Dimension of the inferred joint probability matrix P:
print(ccrvam_obj.P.shape)
# Joint Probability matrix P:
print(ccrvam_obj.P)
```

For higher dimensions, the package supports multi-dimensional NumPy arrays representing contingency tables across multiple predictors. More examples in this regard can be found in @sec-rda.

### Loading from External Data Files

Through `DataProcessor` class in our package (`ccrvam`), we support flexible loading of categorical data in multiple formats, accommodating diverse data structures commonly encountered in statistical analysis. The class provides a unified interface for importing data regardless of its original format, making it accessible for CCRVAM analysis without requiring extensive preprocessing.

The `DataProcessor` class supports three primary data formats:

1.  **Case-form data:** Where each row represents an individual case with categorical variables organized in separate columns, allowing for straightforward representation of raw survey or experimental results. 

2.  **Frequency-form data:** Where each row to contain a unique combination of categorical variables along with their corresponding frequency count, offering a more compact representation when many observations share identical category combinations.

3.  **Table-form data:** Where direct contingency tables are represented as multi-dimensional arrays, which provides the most computationally efficient input format when data has already been aggregated into contingency tables by other statistical software.

The implementation also includes robust handling of several key data management features. The package supports custom variable naming schemes, allowing users to define meaningful labels for their variables rather than relying on default numeric identifiers. It provides automatic mapping of non-integer category values to integer indices, enabling seamless processing of categorical data with text or mixed-type labels. The software accommodates custom delimiters for text-based input files, offering flexibility when importing data from various sources with different formatting conventions. Additionally, the implementation supports dimensional specification for proper array structuring, ensuring that contingency tables are correctly shaped according to the number of categories in each variable, which is essential for accurate computation of the checkerboard copula scores and associated.

This flexible approach to data loading ensures compatibility with various data collection methodologies and storage formats, allowing researchers to focus on analysis rather than data conversion. The `DataProcessor` integrates seamlessly with `GenericCCRVAM` class to initialize model objects directly from imported data, creating a streamlined workflow from raw data to statistical inference. Detailed examples demonstrating each data format, along with corresponding code snippets and implementation considerations, are provided in @sec-rda.

## Checkerboard copula score (especially an ordinal response variable)

Following the theoretical foundation in @def-ccs, the package implements checkerboard copula scores for ordinal variables. These scores represent a transformation that leverages the inherent ordering information in ordinal variables.

The implementation maintains fidelity to the mathematical definitions while providing a computationally efficient vectorized implementation:

```{python}
#| label: calculate-2d-ccs
#| echo: true
#| output: true

# Calculate and display CCS for X1
scores_X1 = ccrvam_obj.calculate_ccs(1)
print(scores_X1)

# Calculate and display Variance of CCS for X1
variance_ccs_X1 = ccrvam_obj.calculate_variance_ccs(1)
print(variance_ccs_X1)
```

Recall that for each ordinal variable $X_j$ with categories $i_j \in \{1, \ldots, I_j\}$, the scores $s_{i_j}^j = (u_{i_j-1}^j + u_{i_j}^j)/2$ are calculated where $u_{i_j}^j$ is defined by the marginal cumulative distribution. This implementation follows directly from the empirical estimation procedure detailed in @sec-empirical-ccs. We can see how the output from the above code-chunk matches with the result in our running example from @sec-ccrvam, which further verifies the reproducible functionality of our package.

`ccrvam` employs vectorized operations through NumPy [@numpy] to ensure computational efficiency, which becomes particularly important for higher-dimensional tables. The variance calculation implements @lem-mean-var-S, providing a measure of dispersion that is essential for the scaled association measures discussed later.

## Checkerboard copula Regression (CCR)

The Checkerboard Copula Regression functionality follows the definition provided in @def-checkerboard-copula-regression, computing the conditional expectation of the copula score for the response variable given values of the predictor variables.

The prediction functionality follows the empirical estimation procedure described in #sec-empirical-ccr, where the predicted category $\hat{x}_{i_j^*}^j$ is determined by finding the interval containing the estimated regression value $\hat{u}_j^* = \hat{r}_{U_j|\mathbf{U}_{-j}}(\hat{\mathbf{u}}_{-j}^*)$.

The `get_category_predictions_ccr()` method performs the essential function of predicting the categories of the response variable (specified through the response input argument) based on given predictor values (enumerated in the predictors input argument). This method implements the core predictive capability of the checkerboard copula regression approach, translating theoretical associations into practical category predictions. The method returns these predictions in an easy-to-read Pandas [@pandas] DataFrame format, making it straightforward for researchers to examine and interpret the results in a familiar tabular structure. Additionally, the method supports custom variable names for enhanced interpretation, allowing users to replace default numeric identifiers with meaningful labels that reflect the actual variables being analyzed in their specific domain context.

The implementation also allows for multiple conditioning axes, supporting complex multivariate analyses, which we can be seen in the examples mentioned in @sec-examples.

```{python}
#| label: calculate-2d-ccr
#| echo: true
#| output: true

# Predictions from X1 to X2:
predictions_X1_to_X2 = ccrvam_obj.get_predictions_ccr(
    predictors=[1], 
    response=2
)
print(predictions_X1_to_X2)

# Example: Showcasing the use of custom variable names for the output
# Predictions from Education Level to Income Bracket:
variable_to_name_dict = {
    1: "Income", 
    2: "Education"
}
predictions_Education_to_Income = ccrvam_obj.get_predictions_ccr(
    predictors=[2], 
    response=1, 
    variable_names=variable_to_name_dict
)
print(predictions_Education_to_Income)
```

The package also provides reference prediction under joint independence, which is important for interpreting the substantive meaning of predictions by comparing against what would be expected if no association existed.

Hence, we can also obtain the response category prediction under the assumption of joint independence between X1 and X2 as follows:

```{python}
#| label: calculate-2d-ccr-jointindep
#| echo: true
#| output: true

# Response category prediction under the joint independence between X1 and X2
print(ccrvam_obj.get_prediction_under_indep(2))
```

## CCR Predicted Category Visualization

The `ccrvam` package includes a comprehensive set of visualization tools for exploring dependence structures in multivariate ordinal data. The package provides a built-in visualization method for CCR predictions:

This visualization approach creates a heatmap-style plot showing the predicted categories of the response variable for different combinations of predictor variable categories. It includes markers for predicted categories and optional reference lines for predictions under joint independence.

The visualization methods support various color schemes for different visual preferences, customizable figure sizes and resolutions, text annotations showing prediction values, different legend styles for handling many predictor combinations, and exportable high-resolution graphics for publications.

These visualizations help researchers understand and communicate the complex dependence structures detected by the CCR approach, making the results more accessible and interpretable.

```{python}
#| label: visualize-2d-ccr
#| echo: true
#| output: true

# Plotting with default naming scheme with tuple labels on x-axis
ccrvam_obj.plot_ccr_predictions(
  predictors=[1],
  response=2,
  legend_style="xaxis"
)

```

```{python}
#| label: visualize-2d-ccr-custom
#| echo: true
#| output: true

# Plotting with custom naming scheme with legend of category combinations
var_names={1:"Income", 2:"Education"}

ccrvam_obj.plot_ccr_predictions(
  predictors=[2],
  response=1,
  legend_style="side",
  variable_names=var_names
)
```

## CCR Prediction Uncertainty Evaluation Using Nonparametric Bootstrap Resampling

To quantify prediction uncertainty, the package implements nonparametric bootstrap methods:

```{python}
#| label: bootstrap-2d-ccr
#| echo: true
#| output: true

from ccrvam import bootstrap_predict_ccr_summary

prediction_matrix = bootstrap_predict_ccr_summary(
    contingency_table_2d,
    predictors=[1],
    predictors_names=["X"],
    response=2,
    response_name="Y",
    n_resamples=9999
)

# Predictions Summary Matrix
print(prediction_matrix)
```

```{python}
#| label: bootstrap-2d-ccr-preds
#| echo: true
#| output: true

print(prediction_matrix.predictions)
```

This implementation follows the nonparametric bootstrap procedure outlined in the previous chapter, where multiple bootstrap samples are generated from the original contingency table, and predictions are made for each resampled dataset. The distribution of predicted categories provides a measure of prediction uncertainty, represented as percentages in the resulting heatmap visualization.

The visualization component employs a color gradient to represent the confidence in predictions, with darker colors indicating higher prediction percentages (greater confidence). Dotted lines indicate predictions under joint independence, providing a reference point for interpretation. More input arguments and options for customization can be explored further in <https://ccrvam.readthedocs.io/>, which hosts detailed documentation for our `ccrvam` package.

```{python}
#| label: visualize-bootstrap-2d-ccr
#| echo: true
#| output: true

# You can also visualize the results with the attached plotting method
prediction_matrix.plot_prediction_heatmap()
```

## (S)CCRAM Estimation

The package implements both the unscaled (CCRAM) and scaled (SCCRAM) versions of the checkerboard copula regression association measure, as defined in @def-CCRAM and @def-SCCRAM:

```{python}
#| label: calculate-2d-sccram
#| echo: true
#| output: true

ccram_X1_to_X2 = ccrvam_obj.calculate_CCRAM(
                    predictors=[1],
                    response=2
                 )
print(f"CCRAM X1 to X2: {ccram_X1_to_X2:.4f}")

sccram_X1_to_X2 = ccrvam_obj.calculate_CCRAM(
                    predictors=[1],
                    response=2,
                    scaled=True
                  )
print(f"SCCRAM X1 to X2: {sccram_X1_to_X2:.4f}")
```

The implementation follows the empirical estimation procedures outlined in the previous chapter, with CCRAM measuring the proportion of variance in the response variable's checkerboard copula score that can be explained by the predictor variables. SCCRAM normalizes this measure to be bounded between 0 and 1, making it easier to interpret and compare across different datasets.

Both measures quantify the strength of even the nonlinear regression relationship between the ordinal response variable and categorical predictors, going beyond traditional correlation measures that primarily detect linear relationships.

## (S)CCRAM Uncertainty Evaluation Using Bootstrap Resampling

As mentioned in @sec-bootstrap-ccram, in order to assess the precision of CCRAM and SCCRAM estimates, the package implements nonparametric bootstrap procedures:

```{python}
#| label: bootstrap-2d-ccram
#| echo: true
#| output: true

from ccrvam import bootstrap_ccram

ccram_result = bootstrap_ccram(
    contingency_table_2d,
    predictors=[1],
    response=2,
    n_resamples=9999,
    scaled=False,
    confidence_level=0.95,
    method="percentile",
    random_state=None
)
```

```{python}
#| label: bootstrap-2d-ccram-metrics
#| echo: true
#| output: true

# Metric Name
print(ccram_result.metric_name)
# Observed Value
print(f"{ccram_result.observed_value:.4f}")
# 95% Confidence Interval
lower_CI_bound = ccram_result.confidence_interval[0]
upper_CI_bound = ccram_result.confidence_interval[1]
print(f"({lower_CI_bound:.4f}, {upper_CI_bound:.4f})")
# Standard Error
print(f"{ccram_result.standard_error:.4f}")
# Bootstrap Estimates
bootstrap_estimates = ccram_result.bootstrap_distribution
print(f"{type(bootstrap_estimates)}")

# Calculate bootstrap bias
bootstrap_mean = np.mean(bootstrap_estimates)
bootstrap_bias = bootstrap_mean - ccram_result.observed_value

# Calculate bootstrap standard error
bootstrap_std_error = np.std(bootstrap_estimates, ddof=1)

# Calculate ratio of bias to standard error
bias_to_se_ratio = bootstrap_bias / bootstrap_std_error

# Additional Bootstrap Statistics
print(f"Bootstrap Mean: {bootstrap_mean:.4f}")
print(f"Bootstrap Bias: {bootstrap_bias:.4f}")
print(f"Bootstrap Standard Error: {bootstrap_std_error:.4f}")
print(f"Bias to Standard Error Ratio: {bias_to_se_ratio:.4f}")
```

The bootstrap procedure generates multiple resamples from the original contingency table, calculates the (S)CCRAM for each resample, and constructs confidence intervals based on the resulting distribution. This provides a measure of the sampling variability and precision of the (S)CCRAM estimate.

The visualization component plots the bootstrap distribution with the observed (S)CCRAM value highlighted, providing a graphical representation of the uncertainty in the estimate. This analysis can be repeated for SCCRAM by setting `scaled = True` as an input argument of the `bootstrap_ccram()` function.

## Statistical Significance Testing for (S)CCRAM Using Permutation Test

As mentioned in @sec-permutation-ccram, in order to assess the statistical significance of CCRAM or SCCRAM, the package implements permutation testing as well. We demonstrate the usage of the same in the case of our 2-D example below:

```{python}
#| label: permutation-2d-ccram
#| echo: true
#| output: true

from ccrvam import permutation_test_ccram

perm_result = permutation_test_ccram(
    contingency_table_2d,
    predictors=[1],
    response=2,
    scaled=False,
    alternative='greater',
    n_resamples=9999
)

```

```{python}
#| label: permutation-2d-ccram-metrics
#| echo: true
#| output: true

print(f"Metric Name: {perm_result.metric_name}")
print(f"Observed Value: {perm_result.observed_value:.4f}")
print(f"P-Value: {perm_result.p_value:.4f}")
# Permutation Distribution
permutation_distribution = perm_result.null_distribution
print(f"Permutation Distribution (Type): {type(permutation_distribution)}")

# Calculate quantiles
q01 = np.quantile(permutation_distribution, 0.01)
# 0.5-th quantile (median)
median = np.median(permutation_distribution)
# 0.99-th quantile
q99 = np.quantile(permutation_distribution, 0.99)

# Calculate interquartile range (IQR)
q25 = np.quantile(permutation_distribution, 0.25)
q75 = np.quantile(permutation_distribution, 0.75)
iqr = q75 - q25

# Permutation Distribution Summary Statistics:
print(f"0.01-th Quantile: {q01:.4f}")
print(f"0.5-th Quantile (Median): {median:.4f}")
print(f"0.99-th Quantile: {q99:.4f}")
print(f"Interquartile Range (IQR): {iqr:.4f}")
```

This implementation follows the permutation testing procedure outlined in the @sec-ccrvam, where the response variable values are randomly permuted to break any association with the predictor variables, thus generating a null distribution under the hypothesis of no association. The p-value is calculated as the proportion of permutation statistics that are as extreme as or more extreme than the observed statistic.

The visualization component plots the null distribution with the observed CCRAM value highlighted, providing a graphical representation of the statistical significance. This analysis can be repeated for SCCRAM by setting `scaled = True` as an input argument of the `permutation_test_ccram()` function.

## Software Architecture and Design Principles

The `ccrvam` Python package was developed following modern software engineering principles to ensure reliability, maintainability, and extensibility. We used `PyPi-Template` [@pypitemplate2024] to initialize the skeleton of our software package.

### Component Structure and Object-Oriented Design

The package follows an object-oriented design, encapsulating related functionality within classes. This design allows users to work with a unified interface while hiding the pesky implementation details from the user, therefore making the package intuitive to use while maintaining flexibility.

The package is organized into three main components:

1.  **Core CCRVAM Implementation** (`GenericCCRVAM` class within `gencopula` module): This is the central object that implements the fundamental calculations for checkerboard copula regression, while handling internal data representation and transformation. Through this, we provide our users several methods for prediction and association measures.

2.  **Statistical Simulation Framework** (`genstatsim` module): This module implements bootstrap and permutation testing procedures, while providing uncertainty quantification for predictions and measures. Through this, we also provide users the flexibility through visualization and exporting methods for statistical results

3.  **Data Processing Utilities** (`utils` module): This module handles user-facing data loading and formatting methods, providing conversion between different data representations such as table form, case form, and frequency form as outlined in @sec-rda. Through this, we also provide users basic functionality for data validation and pre-processing.

We can visualize the package structure and user-experience-workflow through the images below (powered by Mermaid [@mermaid]):

![High-Level Package Structure of CCRVAM](fig/mermaid_pkg_structure.png){#fig-pkgstructure}

@fig-pkgstructure illustrates the modular organization of the `ccrvam` codebase. The core functionality is encapsulated within the checkerboard subpackage, which houses the main analytical engine (`genccrvam.py`), statistical simulation tools (`genstatsim.py`), and data preprocessing utilities (`utils.py`). The top-level package initialization file (`__init__.py`) exposes these modules for external use, while supplementary materials such as documentation, examples, and tests are organized into their respective directories.

![Functional Workflow for Checkerboard Copula Analysis](fig/mermaid_ux_workflow.png){#fig-uxworkflow}

@fig-uxworkflow outlines the end-to-end computational pipeline within the `ccrvam` package. Raw categorical data is processed into a contingency table using the `DataProcessor`. This table can then be passed into the `GenericCCRVAM` class for association analysis, prediction, and visualization. Alternatively, the same input can be used in bootstrapping and permutation testing workflows to produce confidence intervals, p-values, and prediction heatmaps. The system supports both analytical modeling and robust statistical inference.

### Vectorized Implementations, and Error Handling

Performance optimization was a key consideration in the design, particularly for higher-dimensional tables. By leveraging NumPy's [@numpy] vectorized operations, Pandas's [@pandas] effective data-handling, SciPy's [@scipy] bootstrapping function-calls, and Matplotlib's [@matplotlib] efficient graphing APIs (Application Programming Interfaces) the package achieves significantly better performance than naive loop-based implementations, enabling analysis of larger datasets.

The package includes comprehensive input validation and error handling to provide informative messages when issues arise. This approach helps users identify and fix problems quickly, improving the overall user experience. On the developer-side, this allows for easy debugging, and faster development of new features.

## Testing, Validation, and Performance Evaluation

### Comprehensive Test Suite

The `ccrvam` package includes a comprehensive test suite to ensure correctness and reliability across all implemented functionality. The test suite encompasses unit tests for all core functionality, providing verification of individual components in isolation, while integration tests confirm proper behavior in end-to-end workflows that simulate typical user interactions. Edge case testing rigorously examines boundary conditions where algorithms are most likely to fail, and dimensional-invariant testing validates consistent performance across 2D, 3D, and 4D contingency tables of varying complexity. The suite also incorporates regression tests to prevent the reintroduction of previously fixed bugs as the codebase evolves.

The package achieves over 93% code coverage, ensuring that most code paths and user experiences are well-tested and safe for production use in statistical analysis. In order to achieve better observability into our code and maintain a check ensuring that our tests pass irrespective of machine environments, we leveraged pytest [@pytest] and coverage [@coveragepy] Python libraries as our testing infrastructure. These tools provide a robust framework for automated test execution and detailed reporting on test coverage. An example test from our suite is shown below:

```{python}
#| label: test-2d-sccram-metrics
#| eval: false
#| echo: true
#| output: true

@pytest.mark.parametrize(
  "predictors, response, expected_sccram", [
    ([1], 2, 0.84375/(12*0.0703125)), # Single axis X1->X2          
    ([2], 1, 0.0), # Single axis X2->X1                            
  ]
)
def test_calculate_SCCRAM(
  generic_ccrvam, predictors,
  response, expected_sccram
):
    """Test SCCRAM calculations with multiple conditioning axes."""
    calculated = generic_ccrvam.calculate_CCRAM(
                    predictors, response, scaled=True
                 )
    np.testing.assert_almost_equal(calculated, expected_sccram)
```

Additional tests covering various aspects of the package's functionality can be found in the GitHub repository at <https://github.com/DhyeyMavani2003/ccrvam/tree/main/tests>.

### Continuous Integration (CI)

The development workflow includes CI testing through GitHub Actions and `.yml` files. This configuration ensures that tests are run automatically on multiple Python versions (3.8, 3.9, 3.10, 3.11, 3.12, 3.13) for every code change, maintaining compatibility and reliability.

### Performance Benchmarking

The package includes three main performance optimizations for handling larger contingency tables: vectorized implementations for core calculations, caching of intermediate results such as conditional distributions to avoid redundant computation, efficient data structures for sparse representation where appropriate.

For a 4D contingency table (2x3x2x6) with 112 cases, operations like CCRAM calculation and bootstrap simulations complete in seconds on modern hardware, allowing for interactive analysis.

## User Documentation and Example Workflows

The `ccrvam` package includes comprehensive documentation and example workflows to help users get started:

### API Documentation

The package provides detailed API documentation for all user and developer facing functions and classes through Sphinx [@sphinx]. The documentation is hosted on ReadTheDocs at <https://ccrvam.readthedocs.io/>, and include function signatures, input arguments descriptions, outputs documentation, warnings/errors log, usage examples on 2D anf 4D sample datasets, and cross-references to related functions.

### Example Workflows {#sec-examples}

The package includes example workflows to demonstrate common analysis patterns. These examples (located at <https://github.com/DhyeyMavani2003/ccrvam/tree/main/examples/jupyter>) demonstrate complete analysis workflows from data loading to visualization and statistical testing, helping users understand how to apply the package to their own research questions.

In the next chapter, we will use our `ccrvam` package to perform EDA on some real-world datasets.
