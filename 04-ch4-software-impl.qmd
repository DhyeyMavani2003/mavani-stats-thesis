# Software (Package) Implementation and Testing {#sec-software}

```{r}
#| label: setup
#| include: false

# Load knitr package
library(knitr)

# Load packages
library(tidyverse)
library(gt)

# Set default ggplot theme for document
theme_set(theme_classic())
# If using kableExtra tables, print blank cells instead of `NA`
options(knitr.kable.NA = "")
```

In this chapter, we introduce `ccrvam`, a Python package that implements the Checkerboard Copula Regression-based Visualization and Association Measure (CCRVAM) techniques discussed in previous chapters. Despite the growing importance of multivariate categorical data analysis with ordinal response variables in disciplines like medicine, social sciences, and economics, there has been a notable lack of user-friendly, well-tested software implementations that scale efficiently to higher-dimensional problems. The `ccrvam` package addresses this gap by providing a comprehensive suite of tools for analyzing multivariate discrete data using the checkerboard copula approach.

## Set-up and Example Data

The `ccrvam` package is built to analyze multi-dimensional contingency tables with an ordinal response variable and a set of categorical (nominal/ordinal) explanatory variables/predictors. This aligns perfectly with the theoretical frameworks established in @sec-ccrvam. The package is designed with ease of installation and use in mind, particularly within Jupyter Notebook environments common in data analysis workflows.

### Installation

For quick use in Jupyter Notebooks, the package can be installed directly from PyPI with:

```{bash}
#| label: install-bash
#| eval: true
#| echo: true
#| output: false

pip install ccrvam==1.0.0
```

This thesis uses and demonstrates functionality based on version `v1.0.0` of the `ccrvam` package. While this version captures the stable features implemented during the course of this work, the package remains under active development. The most recent release, along with version history and updates, is available on [pypi.org/project/ccrvam](https://pypi.org/project/ccrvam/).

For more containerized production-heavy work, a custom virtual environment is recommended. Instructions for setting that up can be found at [ccrvam/.github/README.md](https://github.com/DhyeyMavani2003/ccrvam?tab=readme-ov-file#readme).

## Types of Input Data Supported

The `ccrvam` package is designed to be flexible with respect to input data formats. The package supports these as the main formats:

### Loading from Contingency Table Format (In-Place)

The `ccrvam` package implements the theoretical framework outlined in @sec-ccrvam by operating on multi-dimensional contingency tables. These tables represent the joint distribution of categorical variables where one variable is explicitly designated as an ordinal response (dependent variable), one or more variables function as categorical predictors (independent variables), and table entries contain frequency counts of observations. The package accepts these contingency tables directly as NumPy arrays, with each dimension corresponding to the categories of a particular variable. This structure allows for efficient computation of the checkerboard copula scores and subsequent association measures while preserving the natural ordering of the response variable categories and accommodating multiple categorical predictors simultaneously.

For example, consider the 2-D example from the previous chapter:

```{python}
#| label: example-init
#| echo: true
#| output: true

import numpy as np
from ccrvam import GenericCCRVAM

# Migraine treatment example (dose vs. pain severity)
contingency_table_2d = np.array([
    [0, 0, 20],
    [0, 10, 0],
    [20, 0, 0],
    [0, 10, 0],
    [0, 0, 20]
])

# Create a CCRVAM object
ccrvam_obj = GenericCCRVAM.from_contingency_table(contingency_table_2d)
# Dimension of the inferred joint probability matrix P:
print(ccrvam_obj.P.shape)
# Joint Probability matrix P:
print(ccrvam_obj.P)
```

For higher dimensions, the package supports multi-dimensional NumPy arrays representing contingency tables across multiple predictors. More examples in this regard can be found in @sec-rda.

### Loading from External Data Files

Through the `DataProcessor` class in our package (`ccrvam`), we support the flexible loading of categorical data in multiple formats, accommodating diverse data structures commonly encountered in statistical analysis. The class provides a unified interface for importing data regardless of its original format, making it accessible for CCRVAM analysis without requiring extensive preprocessing.

The `DataProcessor` class supports three primary data formats:

1.  **Case-form data:** Where each row represents an individual case with categorical variables organized in separate columns, allowing for straightforward representation of raw survey or experimental results. 

2.  **Frequency-form data:** Each row contains a unique combination of categorical variables and their corresponding frequency count, offering a more compact representation when many observations share identical category combinations.

3.  **Table-form data:** Where direct contingency tables are represented as multi-dimensional arrays, providing the most computationally efficient input format when other statistical software aggregates data into contingency tables.

The implementation also includes robust handling of several key data management features. The package supports custom variable naming schemes, allowing users to define meaningful labels for their variables rather than relying on default numeric identifiers. It automatically maps non-integer category values to integer indices, enabling seamless processing of categorical data with text or mixed-type labels. The software accommodates custom delimiters for text-based input files, offering flexibility when importing data from various sources with different formatting conventions. Additionally, the implementation supports the dimensional specification for proper array structuring, ensuring that contingency tables are correctly shaped according to the number of categories in each variable. This is essential for accurate computation of the checkerboard copula scores and associated.

This flexible approach to data loading ensures compatibility with various data collection methodologies and storage formats, allowing researchers to focus on analysis rather than data conversion. The `DataProcessor` integrates seamlessly with the `GenericCCRVAM` class to initialize model objects directly from imported data, creating a streamlined workflow from raw data to statistical inference. Detailed examples demonstrating each data format, corresponding code snippets, and implementation considerations are provided in @sec-rda.

## Checkerboard copula score (for ordinal response variable)

Following the theoretical foundation in @def-ccs, the package implements checkerboard copula scores for ordinal variables. These scores represent a transformation that leverages the inherent ordering information in ordinal variables.

The implementation maintains fidelity to the mathematical definitions while providing a computationally efficient vectorized implementation:

```{python}
#| label: calculate-2d-ccs
#| echo: true
#| output: true

# Calculate and display CCS for X1
scores_X1 = ccrvam_obj.calculate_ccs(1)
print(scores_X1)

# Calculate and display the Variance of CCS for X1
variance_ccs_X1 = ccrvam_obj.calculate_variance_ccs(1)
print(variance_ccs_X1)
```

Recall that for each ordinal variable $X_j$ with categories $i_j \in \{1, \ldots, I_j\}$, the scores $s_{i_j}^j = (u_{i_j-1}^j + u_{i_j}^j)/2$ are calculated where $u_{i_j}^j$ is defined by the marginal cumulative distribution. This implementation follows directly from the empirical estimation procedure detailed in @sec-empirical-ccs. We can see how the output from the above code-chunk matches the result in our running example from @sec-ccrvam, which further verifies the reproducible functionality of our package.

`ccrvam` employs vectorized operations through NumPy [@numpy] to ensure computational efficiency, which becomes particularly important for higher-dimensional tables. The variance calculation implements @lem-mean-var-S, providing a measure of dispersion that is essential for the scaled association measures discussed later.

## Checkerboard copula Regression (CCR)

The Checkerboard Copula Regression functionality follows the definition provided in @def-checkerboard-copula-regression, computing the conditional expectation of the copula score for the response variable given values of the predictor variables.

The prediction functionality follows the empirical estimation procedure described in #sec-empirical-ccr, where the predicted category $\hat{x}_{i_j^*}^j$ is determined by finding the interval containing the estimated regression value $\hat{u}_j^* = \hat{r}_{U_j|\mathbf{U}_{-j}}(\hat{\mathbf{u}}_{-j}^*)$.

The `get_category_predictions_ccr()` method performs the essential function of predicting the categories of the response variable (specified through the response input argument) based on given predictor values (enumerated in the `predictors` input argument). This method implements the core predictive capability of the checkerboard copula regression approach, translating theoretical associations into practical category predictions. The method returns these predictions in an easy-to-read Pandas [@pandas] DataFrame format, making it straightforward for researchers to examine and interpret the results in a familiar tabular structure. Additionally, the method supports custom variable names for enhanced interpretation. It allows users to replace default numeric identifiers with meaningful labels that reflect the actual variables being analyzed in their specific domain context.

The implementation also allows for multiple conditioning axes, supporting complex multivariate analyses, as seen in the examples mentioned in @sec-examples.

```{python}
#| label: calculate-2d-ccr
#| echo: true
#| output: true

# Predictions from X1 to X2:
predictions_X1_to_X2 = ccrvam_obj.get_predictions_ccr(
    predictors=[1], 
    response=2
)
print(predictions_X1_to_X2)
```

```{python}
#| label: calculate-2d-ccr-custom
#| echo: true
#| output: true

# Example: Showcasing the use of custom variable names for the output
# Predictions from Education Level to Income Bracket:
variable_to_name_dict = {
    1: "Income", 
    2: "Education"
}
predictions_Education_to_Income = ccrvam_obj.get_predictions_ccr(
    predictors=[2], 
    response=1, 
    variable_names=variable_to_name_dict
)
print(predictions_Education_to_Income)
```

The package also provides reference prediction under joint independence, which is important for interpreting the substantive meaning of predictions by comparing them against what would be expected if no association existed.

Hence, we can also obtain the response category prediction under the assumption of joint independence between $X_1$ and $X_2$ as follows:

```{python}
#| label: calculate-2d-ccr-jointindep
#| echo: true
#| output: true

# Response category prediction under the joint independence between X1 and X2
print(ccrvam_obj.get_prediction_under_indep(2))
```

## CCR Predicted Category Visualization

The `ccrvam` package includes a comprehensive set of visualization tools for exploring dependence structures in multivariate ordinal data. The package provides a built-in visualization method for CCR predictions:

This visualization approach creates a heatmap-style plot showing the predicted categories of the response variable for different combinations of predictor variable categories. It includes markers for predicted categories and optional reference lines for predictions under joint independence.

The visualization methods support various color schemes for different visual preferences, customizable figure sizes and resolutions, text annotations showing prediction values, different legend styles for handling many predictor combinations, and exportable high-resolution graphics for publications.

These visualizations help researchers understand and communicate the complex dependence structures detected by the CCR approach, making the results more accessible and interpretable.

```{python}
#| label: visualize-2d-ccr
#| echo: true
#| output: true

# Plotting with default naming scheme with tuple labels on the x-axis
ccrvam_obj.plot_ccr_predictions(
  predictors=[1],
  response=2,
  legend_style="xaxis"
)

```

```{python}
#| label: visualize-2d-ccr-custom
#| echo: true
#| output: true

# Plotting with the custom naming scheme with the legend of category combinations
var_names={1: "Income", 2: "Education"}

ccrvam_obj.plot_ccr_predictions(
  predictors=[2],
  response=1,
  legend_style="side",
  variable_names=var_names
)
```

## CCR Prediction Uncertainty Evaluation Using Nonparametric Bootstrap Resampling

To quantify prediction uncertainty, the package implements nonparametric bootstrap methods:

```{python}
#| label: bootstrap-2d-ccr
#| echo: true
#| output: false

from ccrvam import bootstrap_predict_ccr_summary

prediction_matrix = bootstrap_predict_ccr_summary(
    contingency_table_2d,
    predictors=[1],
    predictors_names=["X"],
    response=2,
    response_name="Y",
    n_resamples=9999
)
```

```{python}
#| label: bootstrap-2d-ccr-display
#| echo: true
#| output: true

# Predictions Summary Matrix
print(prediction_matrix)
```

```{python}
#| label: bootstrap-2d-ccr-preds
#| echo: true
#| output: true

print(prediction_matrix.predictions)
```

This implementation follows the nonparametric bootstrap procedure outlined in the previous chapter, where multiple bootstrap samples are generated from the original contingency table, and predictions are made for each resampled dataset. The distribution of predicted categories provides a measure of prediction uncertainty, represented as percentages in the resulting heatmap visualization.

The visualization component employs a color gradient to represent the confidence in predictions, with darker colors indicating higher prediction percentages (greater confidence). Dotted lines indicate predictions under joint independence, providing a reference point for interpretation. More input arguments and options for customization can be explored further in [ccrvam.readthedocs.io](https://ccrvam.readthedocs.io/), which hosts detailed documentation for our `ccrvam` package.

```{python}
#| label: visualize-bootstrap-2d-ccr
#| echo: true
#| output: true

# We can also visualize the results with the attached plotting method
prediction_matrix.plot_prediction_heatmap()
```

## (S)CCRAM Estimation

The package implements both the unscaled (CCRAM) and scaled (SCCRAM) versions of the checkerboard copula regression association measure, as defined in @def-CCRAM and @def-SCCRAM:

```{python}
#| label: calculate-2d-sccram
#| echo: true
#| output: true

ccram_X1_to_X2 = ccrvam_obj.calculate_CCRAM(
                    predictors=[1],
                    response=2
                 )
print(f"CCRAM X1 to X2: {ccram_X1_to_X2:.4f}")

sccram_X1_to_X2 = ccrvam_obj.calculate_CCRAM(
                    predictors=[1],
                    response=2,
                    scaled=True
                  )
print(f"SCCRAM X1 to X2: {sccram_X1_to_X2:.4f}")
```

The implementation follows the empirical estimation procedures outlined in the previous chapter. CCRAM measures the proportion of variance in the response variable's checkerboard copula score that the predictor variables can explain. SCCRAM normalizes this measure to be bounded between 0 and 1, making it easier to interpret and compare across different datasets.

Both measures quantify the strength of even the nonlinear regression relationship between the ordinal response variable and categorical predictors, going beyond traditional correlation measures that primarily detect linear relationships.

## (S)CCRAM Uncertainty Evaluation Using Bootstrap Resampling

As mentioned in @sec-bootstrap-ccram, in order to assess the precision of CCRAM and SCCRAM estimates, the package implements nonparametric bootstrap procedures. `ccrvam` incorporates nonparametric bootstrap procedures based on the `scipy.stats.bootstrap` function [@scipy]. This approach allows users to generate confidence intervals and visualize the empirical distribution of estimated dependence measures, improving inference in discrete multivariate settings.

The bootstrap procedure follows three steps: (1) generating multiple resampled datasets from the original contingency table with replacement; (2) computing the (S)CCRAM statistic for each resample using a custom `ccram_stat` function; and (3) calculating confidence intervals based on the resulting distribution, following the methodology of user's choice ("percentile", "BCa", "basic") from SciPy [@scipy].

```{python}
#| label: bootstrap-2d-ccram
#| echo: true
#| output: true

from ccrvam import bootstrap_ccram

ccram_result = bootstrap_ccram(
    contingency_table_2d,
    predictors=[1],
    response=2,
    n_resamples=9999,
    scaled=False,
    confidence_level=0.95,
    method="percentile",
    random_state=None
)
```

```{python}
#| label: bootstrap-2d-ccram-metrics
#| echo: true
#| output: true

# Metric Name
print(ccram_result.metric_name)
# Observed Value
print(f"{ccram_result.observed_value:.4f}")
# 95% Confidence Interval
lower_CI_bound = ccram_result.confidence_interval[0]
upper_CI_bound = ccram_result.confidence_interval[1]
print(f"({lower_CI_bound:.4f}, {upper_CI_bound:.4f})")
# Standard Error
print(f"{ccram_result.standard_error:.4f}")
# Bootstrap Estimates
bootstrap_estimates = ccram_result.bootstrap_distribution
print(f"{type(bootstrap_estimates)}")

# Calculate bootstrap bias
bootstrap_mean = np.mean(bootstrap_estimates)
bootstrap_bias = bootstrap_mean - ccram_result.observed_value

# Calculate the bootstrap standard error
bootstrap_std_error = np.std(bootstrap_estimates, ddof=1)

# Calculate the ratio of bias to standard error
bias_to_se_ratio = bootstrap_bias / bootstrap_std_error

# Additional Bootstrap Statistics
print(f"Bootstrap Mean: {bootstrap_mean:.4f}")
print(f"Bootstrap Bias: {bootstrap_bias:.4f}")
print(f"Bootstrap Standard Error: {bootstrap_std_error:.4f}")
print(f"Bias to Standard Error Ratio: {bias_to_se_ratio:.4f}")
```

The bootstrap procedure generates multiple resamples from the original contingency table, calculates each resample's (S)CCRAM, and constructs confidence intervals based on the resulting distribution. This provides a measure of the sampling variability and precision of the (S)CCRAM estimate. The implementation builds directly on SciPy's `bootstrap` function, which allows users to specify the statistic of interest as a callable and automatically handles paired resampling, confidence interval construction, and standard error estimation. We set `vectorized=True` during our SciPy API call to enable fast computation over multiple resamples. As for the above example, we use `method="percentile"` to derive confidence intervals from the empirical quantiles of the bootstrap distribution. But user is allowed to use `"basic"` or `"BCa"` depending on their needs and preferences.

The visualization component plots the bootstrap distribution with the observed (S)CCRAM value highlighted, providing a graphical representation of the uncertainty in the estimate. The auto-plotting in the `CustomBootstrapResult` object highlights the observed value within the distribution, offering a visual representation of uncertainty.

This analysis can be repeated for SCCRAM by setting `scaled = True` as an input argument of the `bootstrap_ccram()` function.

## Statistical Significance Testing for (S)CCRAM Using Permutation Test

As mentioned in @sec-permutation-ccram, the package also implements permutation testing to assess the statistical significance of CCRAM or SCCRAM. We use permutation testing functionality `scipy.stats.permutation_test` from SciPy [@scipy]. This procedure randomly permutes the response labels while keeping the predictor structure fixed, forming a null distribution against which the observed statistic is compared.

We demonstrate the usage of the same within our package `ccrvam` in the case of the 2-D example below:

```{python}
#| label: permutation-2d-ccram
#| echo: true
#| output: true

from ccrvam import permutation_test_ccram

perm_result = permutation_test_ccram(
    contingency_table_2d,
    predictors=[1],
    response=2,
    scaled=False,
    alternative='greater',
    n_resamples=9999
)

```

```{python}
#| label: permutation-2d-ccram-metrics
#| echo: true
#| output: true

print(f"Metric Name: {perm_result.metric_name}")
print(f"Observed Value: {perm_result.observed_value:.4f}")
print(f"P-Value: {perm_result.p_value:.4f}")
# Permutation Distribution
permutation_distribution = perm_result.null_distribution
print(f"Permutation Distribution (Type): {type(permutation_distribution)}")

# Calculate quantiles
q01 = np.quantile(permutation_distribution, 0.01)
# 0.5-th quantile (median)
median = np.median(permutation_distribution)
# 0.99-th quantile
q99 = np.quantile(permutation_distribution, 0.99)

# Calculate interquartile range (IQR)
q25 = np.quantile(permutation_distribution, 0.25)
q75 = np.quantile(permutation_distribution, 0.75)
iqr = q75 - q25

# Permutation Distribution Summary Statistics:
print(f"0.01-th Quantile: {q01:.4f}")
print(f"0.5-th Quantile (Median): {median:.4f}")
print(f"0.99-th Quantile: {q99:.4f}")
print(f"Interquartile Range (IQR): {iqr:.4f}")
```

This implementation follows the permutation testing procedure outlined in the @sec-ccrvam, where the response variable values are randomly permuted to break any association with the predictor variables, thus generating a null distribution under the hypothesis of no association. The p-value is calculated as the proportion of permutation statistics that is as extreme as or more extreme than the observed statistic. The permutation test is powered by SciPy's `permutation_test` utility [@scipy]. By specifying `permutation_type="pairings"` in our SciPy API call, we ensure that only the response values are shuffled, preserving the dependency structure among predictors. The test statistic is defined through a wrapper around the CCRAM computation, and the passage of `vectorized=True` in our SciPy API call enables batch execution across resamples.

The visualization component plots the null distribution with the observed CCRAM value highlighted, providing a graphical representation of the statistical significance. As for the above example, we use `alternative="greater"` for our hypothesis testing. But user is allowed to use `"two-sided"` or `"less"` depending on their needs and preferences.

This analysis can be repeated for SCCRAM by setting `scaled = True` as an input argument of the `permutation_test_ccram()` function.

## Software Architecture and Design Principles

The `ccrvam` Python package was developed following modern software engineering principles to ensure reliability, maintainability, and extensibility. We used `PyPi-Template` [@pypitemplate2024] to initialize the skeleton of our software package.

### Component Structure and Object-Oriented Design

The package follows an object-oriented design, encapsulating related functionality within classes. This design allows users to work with a unified interface while hiding the pesky implementation details from the user, therefore making the package intuitive to use while maintaining flexibility.

The package is organized into three main components:

1.  **Core CCRVAM Implementation** (`GenericCCRVAM` class within the `gencopula` module): This central object implements the fundamental calculations for checkerboard copula regression while handling internal data representation and transformation. Through this, we provide our users with several methods for estimating prediction and association measures.

2.  **Statistical Simulation Framework** (`genstatsim` module): This module implements bootstrap and permutation testing procedures while providing uncertainty quantification for predictions and measures. Through this, we also provide users flexibility through visualization and exporting methods for statistical results

3.  **Data Processing Utilities** (`utils` module): This module handles user-facing data loading and formatting methods, providing conversion between data representations such as table form, case form, and frequency form as outlined in @sec-rda. Through this, we also provide users with basic data validation and preprocessing functionality.

We can visualize the package structure and user-experience-workflow through the images below powered by Mermaid [@mermaid]:

![High-Level Package Structure of CCRVAM](fig/mermaid_pkg_structure_final.png){#fig-pkgstructure fig-align="left" height=500% width=110%}

@fig-pkgstructure illustrates the modular organization of the `ccrvam` codebase. The core functionality is encapsulated within the checkerboard subpackage, which houses the main analytical engine (`genccrvam.py`), statistical simulation tools (`genstatsim.py`), and data preprocessing utilities (`utils.py`). The top-level package initialization file (`__init__.py`) exposes these modules for external use, while supplementary materials such as documentation, examples, and tests are organized into their respective directories.

![Functional Workflow for Checkerboard Copula Analysis](fig/mermaid_ux_workflow_final.png){#fig-uxworkflow fig-align="left" height=500% width=110%}

@fig-uxworkflow outlines the end-to-end computational pipeline within the `ccrvam` package. Raw categorical data is processed into a contingency table using the `DataProcessor`. This table can then be passed into the `GenericCCRVAM` class for association analysis, prediction, and visualization. Alternatively, the same input can be used in bootstrapping and permutation testing workflows to produce confidence intervals, p-values, and prediction heatmaps. The system supports both analytical modeling and robust statistical inference.

### Vectorized Implementations and Error Handling

Performance optimization was a key consideration in the design, particularly for higher-dimensional tables. By leveraging NumPy's [@numpy] vectorized operations, Pandas's [@pandas] effective data handling, SciPy's [@scipy] bootstrapping function calls, and Matplotlib's [@matplotlib] efficient graphing APIs (Application Programming Interfaces) the package achieves significantly better performance than naive loop-based implementations, enabling analysis of larger datasets.

The package includes comprehensive input validation and error handling to provide informative messages when issues arise. This approach helps users identify and fix problems quickly, improving the overall user experience. On the developer side, this allows for easy debugging and faster development of new features.

## Testing, Validation, and Performance Evaluation

### Comprehensive Test Suite

The `ccrvam` package includes a comprehensive test suite to ensure correctness and reliability across all implemented functionality. The test suite encompasses unit tests for all core functionality, verifying individual components in isolation. In contrast, integration tests confirm proper behavior in end-to-end workflows that simulate typical user interactions. Edge case testing rigorously examines boundary conditions where algorithms are most likely to fail, and dimensional-invariant testing validates consistent performance across varying complexity across 2D, 3D, and 4D contingency tables. The suite also incorporates regression tests to prevent reintroducing previously fixed bugs as the codebase evolves.

The package achieves over 93% code coverage, ensuring that most code paths and user experiences are well-tested and safe for production use in statistical analysis. In order to achieve better observability in our code and maintain a check ensuring that our tests pass irrespective of machine environments, we leveraged pytest [@pytest] and coverage [@coveragepy] Python libraries as our testing infrastructure. These tools provide a robust framework for automated test execution and detailed reporting on test coverage. An example test from our suite is shown below:

```{python}
#| label: test-2d-sccram-metrics
#| eval: false
#| echo: true
#| output: true

@pytest.mark.parametrize(
  "predictors, response, expected_sccram", [
    ([1], 2, 0.84375/(12*0.0703125)), # Single axis X1->X2          
    ([2], 1, 0.0), # Single axis X2->X1                            
  ]
)
def test_calculate_SCCRAM(
  generic_ccrvam, predictors,
  response, expected_sccram
):
    """Test SCCRAM calculations with multiple conditioning axes."""
    calculated = generic_ccrvam.calculate_CCRAM(
                    predictors, response, scaled=True
                 )
    np.testing.assert_almost_equal(calculated, expected_sccram)
```

Additional tests covering various aspects of the package's functionality can be found in the GitHub repository at [ccrvam/tests](https://github.com/DhyeyMavani2003/ccrvam/tree/main/tests).

### Continuous Integration (CI)

To ensure code reliability and maintain compatibility across environments, the development workflow for `ccrvam` incorporates automated Continuous Integration (CI) using **GitHub Actions**. The CI pipeline is configured via `.yml` files to trigger test suites automatically upon every commit or pull request.

This setup enables testing across multiple Python versions—specifically 3.8, 3.9, 3.10, 3.11, 3.12, and 3.13—ensuring that the package remains robust and backward-compatible with older versions while embracing newer releases. Each test run verifies the integrity of core functionalities, including CCRAM/SCCRAM computations, bootstrapping procedures, and parallelization logic, helping to prevent regressions and promote long-term maintainability.

By integrating CI early in the development cycle, the package maintains high reliability and reproducibility throughout its evolution.

### Performance Benchmarking

To ensure scalability and responsiveness when working with complex, high-dimensional contingency tables, the `ccrvam` package incorporates a series of optimizations for computational efficiency and usability. Three core strategies are employed: (1) **vectorized implementations** for all core computations to eliminate Python-level loops; (2) **caching of intermediate statistics**, such as conditional distributions, to avoid redundant recalculations; and (3) **sparse-aware data structures**, which reduce memory overhead and accelerate access for large and sparse tables.

For example, in a contingency table of shape 2×3×2×6 (112 total cells), both the calculation of the Scaled CCRAM (SCCRAM) metric and the generation of bootstrap-based confidence intervals typically complete in under a few seconds on a modern CPU. This efficiency enables near real-time analysis of moderately sized categorical datasets.

A key component of the `ccrvam` package is the `bootstrap_predict_ccr_summary()` function, which estimates predictive performance across all combinations of predictor values via a bootstrap resampling procedure. This function computes a summary matrix of prediction percentages, showing the proportion of bootstrap samples assigned to each possible response category for each predictor combination. Notably, this method is designed with **native support for parallel execution**. By simply setting the `parallel=True` flag (enabled by default), users can leverage multiple CPU cores to accelerate computation. Internally, `ccrvam` utilizes Python’s `concurrent.futures.ProcessPoolExecutor` to automatically detect and allocate available cores, executing bootstrap iterations independently and concurrently. Empirical benchmarks demonstrate the efficiency of this design: enabling parallelism yields a **3× reduction in runtime latency with 7 threads**, and up to an **15× speedup using 63 cores on FrostByte**, Amherst College’s High Performance Computing (HPC) cluster. This substantial performance gain makes `ccrvam` highly scalable and well-suited for large-scale studies requiring intensive resampling procedures.

Finally, to enhance computational performance on larger datasets, `ccrvam` is implemented in such a way to allow for seamless GPU integration on the user side at the moment. By leveraging [CuPy](https://github.com/cupy/cupy), a NumPy-compatible library for GPU-accelerated computing [@cupy], users can offload core computations to NVIDIA CUDA or AMD ROCm GPUs with minimal code modifications. This enables significant speedups in parallelizable operations such as bootstrapping and marginalization over large multi-way tables.

## User Documentation and Example Workflows

The `ccrvam` package includes comprehensive documentation and example workflows to help users get started:

### API Documentation

The package provides detailed API documentation for all user and developer-facing functions and classes through Sphinx [@sphinx]. The documentation is hosted on ReadTheDocs at [ccrvam.readthedocs.io](https://ccrvam.readthedocs.io). It includes function signatures, input argument descriptions, outputs documentation, warnings/errors log, usage examples on 2D and 4D sample datasets, and cross-references to related functions.

### Example Workflows {#sec-examples}

The package includes example workflows to demonstrate common analysis patterns. These examples (located at [ccrvam/examples/jupyter](https://github.com/DhyeyMavani2003/ccrvam/tree/main/examples/jupyter)) demonstrate complete analysis workflows from data loading to visualization and statistical testing, helping users understand how to apply the package to their own research questions.

In the next chapter, we will use our `ccrvam` package to perform EDA on some real-world datasets.
