{"title":"Unraveling Notion of Dependence through Copulas","markdown":{"headingText":"Unraveling Notion of Dependence through Copulas","headingAttr":{"id":"sec-simple","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n```{r}\n#| label: setup\n#| include: false\n\n# Load knitr package\nlibrary(knitr)\n\n# Python Engine Setup\nknit_engines$set(python3 = knit_engines$get(\"python\"))\n\n# Load packages\nlibrary(tidyverse)\nlibrary(gt)\n\n# Set default ggplot theme for document\ntheme_set(theme_classic())\n# If using kableExtra tables, print blank cells instead of `NA`\noptions(knitr.kable.NA = \"\")\n\n# Load NBA Data\nload(\"data/temp_wnba.RData\")\n```\n\n<!-- Make it accessible by starting with a motivating example, then proceed towards more rigorousness gradually until challenges for discrete case are covered. -->\n\nIn this chapter, we aim to formalize concepts of dependence and association. To facilitate our understanding, we will use two bivariate random vectors and visualize their relationships through Python code.\n\n## Motivating Example\n\nConsider $(X_1, X_2)$ and $(Y_1, Y_2)$ be bivariate random vectors, each consisting of 10000 independent data-points, which are distributed with the joint distributions $F_X$ and $F_Y$ respectively. Given these bivariate vectors, one might ask: How can I compare the relationship between $(X_1,X_2)$ to the relationship between $(Y_1, Y_2)$? One of the measures that can help us compare and contrast these relationships is Pearson correlation coefficient (commonly denoted as $\\rho_{pearson}$). After preliminary calculations on a Python3 kernel, we can see that $\\rho_{pearson}(X_1, X_2) \\approx 0.802$, but on the other hand, the correlation between $\\rho_{pearson}(Y_1, Y_2) \\approx 0.755$. From these measure-values, it seems that the dependence between $(X_1,X_2)$ is stronger than the dependence between $(Y_1, Y_2)$. Although this agrees with our scatter plots in @fig-motivating-example, it is vital to note that $\\rho_{pearson}$ only captures the linear dependence between the underlying random variables at hand.\n\nUpon observing the @fig-motivating-example closely, we note that the marginal distributions of $X_1$ and $X_2$ are close to normal, unlike the marginals of $Y_1$ and $Y_2$. Moreover, we can see that the relationship between $Y_1$ and $Y_2$ is non-linear. This vast difference in marginals takes away our trust from the appropriateness of the use of $\\rho_{pearson}$ as a measure to compare dependence between the data vectors at hand.\n\n```{python3}\n#| label: gen-data-motivating-example\n#| echo: false\n#| output: false\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta, expon, norm, gamma, binom\nimport os\n\n# Create directory if not exists\nfig_dir = \"fig\"\nos.makedirs(fig_dir, exist_ok=True)\n\n# Generate Data\nnp.random.seed(8990)\nn = 10000\nmean = [0, 0]\ncov = [[1, 0.8], [0.8, 1]]\nX = np.random.multivariate_normal(mean, cov, size=n)\nX1, X2 = X[:, 0], X[:, 1]\n\n# Transform U_X1 and U_X2 to uniform [0, 1] using the CDF of the normal distribution\nU_X1 = norm.cdf(X1)\nU_X2 = norm.cdf(X2)\n\n# Transform U_X1 and U_X2 into Gamma and Beta distributions\nY1 = gamma.ppf(U_X1, a=3, scale=1/15) \nY2 = beta.ppf(U_X2, a=5, b=3)\n\n# Calculate Pearson Correlation Coefficients\nrho_X = np.corrcoef(X1, X2)[0, 1]\nrho_Y = np.corrcoef(Y1, Y2)[0, 1]\nprint(\"Pearson correlation for (X1, X2):\", rho_X)\nprint(\"Pearson correlation for (Y1, Y2):\", rho_Y)\n\n# Create Layout design and Set Size-Ratio\nfig, axes = plt.subplots(1, 2, figsize=(6, 4))\n\n# Scatter plot for (X1, X2)\naxes[0].scatter(X1, X2, alpha=0.3, s=5)\naxes[0].set_title(\"Scatter plot of (X1, X2)\")\naxes[0].set_xlabel(\"X1\")\naxes[0].set_ylabel(\"X2\")\n\n# Add marginal histograms\naxes[0].hist(X1, bins=50, density=True, alpha=0.9, color='blue', orientation='vertical', histtype='step')\naxes[0].hist(X2, bins=50, density=True, alpha=0.9, color='red', histtype='step', orientation='horizontal')\n\n# Scatter plot for (Y1, Y2)\naxes[1].scatter(Y1, Y2, alpha=0.3, s=5)\naxes[1].set_title(\"Scatter plot of (Y1, Y2)\")\naxes[1].set_xlabel(\"Y1\")\naxes[1].set_ylabel(\"Y2\")\n\n# Add marginal histograms\naxes[1].hist(Y1, bins=50, density=True, alpha=0.9, color='blue', orientation='vertical', histtype='step')\naxes[1].hist(Y2, bins=50, density=True, alpha=0.9, color='red', histtype='step', orientation='horizontal')\n\n# Organize into a tight layout as per matplotlib\nplt.tight_layout()\n\n# Save figure instead of showing it\nfig_path = os.path.join(fig_dir, \"motivating_example.png\")\nplt.savefig(fig_path, dpi=300, bbox_inches='tight')\n\n# Close the figure to prevent rendering output\nplt.close(fig)  \n```\n\n```{r}\n#| label: fig-motivating-example\n#| fig-cap: \"Scatter plots of 10000 independent observations of $(X_1, X_2)$ and $(Y_1, Y_2)$ with overlaid curves depicting respective marginal distributions.\"\n\nknitr::include_graphics(\"fig/motivating_example.png\")\n\n```\n\nLet's introduce a lemma that will help us transform the marginals so that the resulting marginals are more similar, and try to only capture or extract the \"dependence\" components, which will allow us to make fairer comparisons. \n\n::: {#lem-prob-int-trans}\n## Probability Integral Transformation\n[@ecmr]\nLet $F$ be a continuous distribution function and let $X \\sim F$, then $F(X)$ is a standard uniform random variable, that is, $F(X) \\sim U(0,1)$.\n:::\n\n@lem-prob-int-trans allows us to transform a continuous random variable to a random variable which has standard uniform distribution. So, by using this transformation, we can now convert our marginals $X_1, X_2, Y_1, Y_2$ individually to be distributed $\\text{Uniform}(0,1)$. And, since now the resulting marginals will all be of the same type, it will allow us to compare the dependence between random variables on fairer grounds.\n\nFor instance, if we know that $X_1 \\sim N(0,1) = F_1$, $X_2 \\sim N(0,1) = F_2$, $Y_1 \\sim Gamma(3, 15) = G_1$, and $Y_2 \\sim Beta(5, 3) = G_2$, where $F_1, F_2, G_1, G_2$ denote the distribution functions of the respective random variables. By @lem-prob-int-trans, we can say that $F_1(X_1), F_2(X_2), G_1(Y_1),$ and $G_2(Y_2)$ are each distributed $\\text{Uniform}(0,1)$.\n\n```{python3}\n#| label: gen-data-transformed-motivating-example\n#| echo: false\n#| output: false\n\n# Set random seed for reproducibility\nnp.random.seed(8990)\n\n# Apply probability integral transformation to all variables to make them uniform\nU_Y1 = gamma.cdf(Y1, a=3, scale=1/15)\nU_Y2 = beta.cdf(Y2, a=5, b=3)\n\n# Calculate Pearson Correlation Coefficients\nrho_U_X = np.corrcoef(U_X1, U_X2)[0, 1]\nrho_U_Y = np.corrcoef(U_Y1, U_Y2)[0, 1]\nprint(\"Pearson correlation for ($F_1(X_1)$, $F_2(X_2)$):\", rho_U_X)\nprint(\"Pearson correlation for ($G_1(Y_1)$, $G_2(Y_2)$):\", rho_U_Y)\n\n# Combine transformed data\nuniform_data = np.vstack([U_X1, U_X2, U_Y1, U_Y2]).T\n\n# Verify the uniformity of transformed data (Should be 0.5 in value)\nprint(\"U_X1 mean:\", U_X1.mean(), \"U_X2 mean:\", U_X2.mean())\nprint(\"U_Y1 mean:\", U_Y1.mean(), \"U_Y2 mean:\", U_Y2.mean())\n\n# Create Layout design and Set Size-Ratio\nfig, axes = plt.subplots(1, 2, figsize=(6, 4))\n\n# Scatter plot for (U_X1, U_X2)\naxes[0].scatter(U_X1, U_X2, alpha=0.3, s=5)\naxes[0].set_title(\"Scatter plot of ($F_1(X_1)$, $F_2(X_2)$)\")\naxes[0].set_xlabel(\"$F_1(X_1)$\")\naxes[0].set_ylabel(\"$F_2(X_2)$\")\n\n# Add marginal histograms\naxes[0].hist(U_X1, bins=50, density=True, alpha=0.9, color='blue', orientation='vertical', histtype='step')\naxes[0].hist(U_X2, bins=50, density=True, alpha=0.9, color='red', histtype='step', orientation='horizontal')\n\n# Scatter plot for (U_Y1, U_Y2)\naxes[1].scatter(U_Y1, U_Y2, alpha=0.3, s=5)\naxes[1].set_title(\"Scatter plot of ($G_1(Y_1)$, $G_2(Y_2)$)\")\naxes[1].set_xlabel(\"$G_1(Y_1)$\")\naxes[1].set_ylabel(\"$G_2(Y_2)$\")\n\n# Add marginal histograms\naxes[1].hist(U_Y1, bins=50, density=True, alpha=0.9, color='blue', orientation='vertical', histtype='step')\naxes[1].hist(U_Y2, bins=50, density=True, alpha=0.9, color='red', histtype='step', orientation='horizontal')\n\n# Organize into a tight layout as per matplotlib\nplt.tight_layout()\n\n# Save figure instead of showing it\nfig_path = os.path.join(fig_dir, \"transformed_motivating_example.png\")\nplt.savefig(fig_path, dpi=300, bbox_inches='tight')\n\n# Close the figure to prevent rendering output\nplt.close(fig)  \n```\n\n```{r}\n#| label: fig-transformed-motivating-example\n#| fig-cap: \"Scatter plots of 10000 independent observations of ($F_1(X_1)$, $F_2(X_2)$) and ($G_1(Y_1)$, $G_2(Y_2)$) with overlaid curves depicting respective marginal distributions.\"\n\nknitr::include_graphics(\"fig/transformed_motivating_example.png\")\n\n```\n\nLooking at @fig-transformed-motivating-example, we can see that the transformed data vectors appear to be significantly similar. We can computationally verify this by quickly calculating the $\\rho_{pearson}$ for ($F_1(X_1)$, $F_2(X_2)$) and ($G_1(Y_1)$, $G_2(Y_2)$), which turns out to be 0.788 for both data vector pairs, meaning that both have same dependence structures.\n\nAn alternative way to approach the problem (of comparing dependence of distinct pairs of marginals), is by transforming the marginals of $(Y_1, Y_2)$ to be normal (same as marginals of $(X_1, X_2)$). As one can predict, in order to accomplish this transformation, we would need to \"undo the current distributional mappings on $(Y_1, Y_2)$\", which we can formally define as generalized inverse as follows:\n\n::: {#def-quantile-function}\n## Quantile Function\n[@ecmr]\n$F^{\\leftarrow}$ (Quantile Function) is defined as $F^{\\leftarrow}(y) = \\text{inf} \\{x \\in \\mathbb{R} | F(x) \\geq y \\}$, where $y \\in [0, 1]$, and $\\text{inf}$ is the infimum of a set.\n:::\n\n:::{.callout-warning}\nThe quantile function $F^{\\leftarrow} = F^{-1}$ only when $F$ is continuous and strictly increasing. Thus it is important to note that, in other cases, the ordinary inverse $F^{-1}$ need not exist. [@ecmr]\n:::\n\nWith the above definition of $F^{\\leftarrow}$, let's introduce a lemma from [@ecmr] that will help us perform the transformation to normal.\n\n::: {#lem-quantile-transformation}\n## Quantile Transformation\n[@ecmr]\nLet $U \\sim \\text{Unif}(0, 1)$ and let $F$ be any distribution function be a distribution function. Then $F^{\\leftarrow}(U) \\sim F$, that is, $F^{\\leftarrow}(X)$ is distributed with density $F$.\n:::\n\n:::{.callout-note}\n@lem-quantile-transformation is valid for non-continuous densities $F$ as well. [@ecmr]\n:::\n\nLet's start with the transformations where we left off in @fig-transformed-motivating-example, since we have uniform densities there. Applying @lem-quantile-transformation on $G_1(Y_1)$ and $G_2(Y_2)$ using quantile functions $F_1^{\\leftarrow} = F_1^{-1}$ and $F_2^{\\leftarrow} = F_2^{-1}$ respectively gives us that $F_1^{-1}(G_1(Y_1)) \\sim F_1$ and $F_2^{-1}(G_2(Y_2)) \\sim F_2$. \n\n```{python3}\n#| label: gen-data-quantile-transformed-motivating-example\n#| echo: false\n#| output: false\n\n# Set random seed for reproducibility\nnp.random.seed(8990)\n\n# Transform (Y1, Y2) back to normal marginals using quantile transformation\nF1_Y1 = norm.ppf(gamma.cdf(Y1, a=3, scale=1/15))\nF2_Y2 = norm.ppf(beta.cdf(Y2, a=5, b=3))\n\n# Calculate Pearson Correlation Coefficients\nrho_F_Y = np.corrcoef(F1_Y1, F2_Y2)[0, 1]\nprint(\"Pearson correlation for transformed:\", rho_F_Y)\nprint(\"Pearson correlation between X1 and X2:\", rho_X)\n\n# Plot the scatter plots with marginal histograms\nfig, axes = plt.subplots(1, 2, figsize=(6, 4))\n\n# Scatter plot for original normal marginals (X1, X2)\naxes[0].scatter(X1, X2, alpha=0.3, s=10)\naxes[0].set_title(\"Scatter plot ($F_1(X_1)$, $F_2(X_2)$)\")\naxes[0].set_xlabel(\"$F_1(X_1)$\")\naxes[0].set_ylabel(\"$F_2(X_2)$\")\naxes[0].hist(X1, bins=50, density=True, alpha=0.6, color='blue', histtype='step')\naxes[0].hist(X2, bins=50, density=True, alpha=0.6, color='red', histtype='step', orientation='horizontal')\n\n# Scatter plot for transformed normal marginals (F1_Y1, F2_Y2)\naxes[1].scatter(F1_Y1, F2_Y2, alpha=0.3, s=10)\naxes[1].set_title(\"Scatter plot ($F_1^{-1}(G_1(Y_1))$, $F_2^{-1}(G_2(Y_2))$)\")\naxes[1].set_xlabel(\"$F_1^{-1}(G_1(Y_1))$\")\naxes[1].set_ylabel(\"$F_2^{-1}(G_2(Y_2))$\")\naxes[1].hist(F1_Y1, bins=50, density=True, alpha=0.6, color='blue', histtype='step')\naxes[1].hist(F2_Y2, bins=50, density=True, alpha=0.6, color='red', histtype='step', orientation='horizontal')\n\n# Layout adjustment and save the figure\nplt.tight_layout()\nfig_path = os.path.join(fig_dir, \"quantile_transformed_motivating_example.png\")\nplt.savefig(fig_path, dpi=300, bbox_inches=\"tight\")\n\n# Close the figure to prevent rendering output\nplt.close(fig)  \n\n```\n\n```{r}\n#| label: fig-quantile-transformed-motivating-example\n#| fig-cap: \"Scatter plots of 10000 independent observations of ($X_1$, $X_2$) and ($F_1^{-1}(G_1(Y_1))$, $F_2^{-1}(G_2(Y_2))$) with overlaid curves depicting respective marginal distributions.\"\n\nknitr::include_graphics(\"fig/quantile_transformed_motivating_example.png\")\n\n```\n\nNotice in @fig-quantile-transformed-motivating-example that the resulting transformed distribution through this alternative method resembles that of $(X_1, X_2)$. Hence, we can conclude that they have the same dependence. Furthermore, through a quick calculation, we can see that $\\rho_{pearson}(F_1^{-1}(G_1(Y_1)), F_2^{-1}(G_2(Y_2))) = 0.802$, which is the same as the Pearson correlation coefficient between $X_1$ and $X_2$. This is the level of flexibility that a combination of transformations presented in @lem-prob-int-trans and @lem-quantile-transformation can lend us.\n\n:::{.callout-note}\n\"$(X_1, X_2)$ and $(Y_1, Y_2)$ have the same dependence\" $\\iff$ \"$(X_1, X_2)$ and $(Y_1, Y_2)$ have the same copula\" [@ecmr]\n:::\n\n## Copulas: A Unified Framework for Dependence\n\nCopulas are a class of multivariate distribution functions with $Unif(0, 1)$ marginals. The motivating example in the previous section explains the usage of copulas as the structures capturing margin-independent dependence between random variables.\n\n:::{.callout-note}\nThe choice of $Unif(0, 1)$ as a post-transformation margin for the data at hand is  somewhat arbitrary although it does simplify further results. One can use modifications of @lem-prob-int-trans and @lem-quantile-transformation to define copulas with respect to any margin of choice without affecting the final conclusions about the dependence between the data at hand. [@ecmr]\n:::\n\nIn order to understand copulas better, for now, let's restrict ourselves to the 2-D (2-dimensional) case. Firstly, let's introduce the definition of a broader class of functions called subcopulas as a preliminary, which will help us mathematically define copulas as a special case. [@nelsen]\n\n::: {#def-2d-subcopula}\n## 2-Dimensional Subcopula\n[@Erdely2017]\nA **two-dimensional subcopula** (2-subcopula) is a function $C^S: D_1 \\times D_2 \\to [0, 1]$, where $\\{0, 1\\} \\subseteq D_i \\subseteq [0, 1]$ for $i \\in \\{1, 2\\}$ with the following conditions satisfied:\n\n* _Grounded:_ $C^S(u, 0) = 0 = C^S(0, v)$, $\\forall u \\in D_1, \\forall v \\in D_2$.\n\n* _Marginal Consistency:_ $\\forall u \\in D_1$ and $\\forall v \\in D_2$, $C^S(u, 1) = u$ and $C^S(1, v) = v$.\n\n* _2-increasing:_ $\\forall u_1,u_2 \\in D_1$ and $\\forall v_1, v_2 \\in D_2$ such that $u_1 \\leq u_2$ and $v_1 \\leq v_2$, $C^S(u_1, v_1) - C^S(u_2, v_1) + C^S(u_2, v_2) - C^S(u_1, v_2) \\geq 0$.\n:::\n\n::: {#def-2d-copula}\n## 2-Dimensional Copula\n[@Erdely2017]\nA **two-dimensional copula** (2-copula) is a function $C: [0, 1] \\times [0, 1] \\to [0, 1]$, with the following conditions satisfied:\n\n* _Grounded:_ $C(u, 0) = 0 = C(0, v)$, $\\forall u \\in [0, 1], \\forall v \\in [0, 1]$.\n\n* _Marginal Consistency:_ $\\forall u \\in [0, 1]$ and $\\forall v \\in [0, 1]$, $C(u, 1) = u$ and $C(1, v) = v$.\n\n* _2-increasing:_ $\\forall u_1,u_2 \\in [0, 1]$ and $\\forall v_1, v_2 \\in [0, 1]$ such that $u_1 \\leq u_2$ and $v_1 \\leq v_2$, $C(u_1, v_1) - C(u_2, v_1) + C(u_2, v_2) - C(u_1, v_2) \\geq 0$.\n:::\n\n:::{.callout-note}\nA 2-D copula is essentially a 2-subcopula with a full unit square as domain ($D_1 = D_2 = [0, 1]$). Furthermore, copula and subcopula are the same within a domain with continuous variables. Later in this chapter, we will discuss why this doesn't hold when one of the variables is discrete. \n:::\n\nIn this work, we will mainly deal with 2-D copulas and subcopulas, but the definitions above can be generalized to n-D case with some notable exceptions detailed (with proofs) in section 2.10 of @nelsen. Moreover, there are many different families of copulas bearing peculiar properties and corresponding margins, we are not covering them in detail since that is not the focus of this work, and a comprehensive summary of many of these families can be found in chapter 3 of @ecmr.\n\n## Fréchet-Hoeffding Bounds\n\nFor any distribution function, boundedness is always a desired property. In the case of copulas, we have a famous theorem that provides us the upper and lower pointwise bounds.\n\n::: {#thm-fh-bounds}\n## Fréchet-Hoeffding Bounds\n[@ecmr]\nGiven a 2-D copula $C$, $W(u,v) = \\max\\{0, u+v-1\\} \\leq C(u,v) \\leq \\min\\{u,v\\} = M(u,v)$, where $u, v \\in [0,1]$.\n:::\n\n## Sklar's Theorem and its Corollaries\n\n@thm-sklar by [@sklar1959fonctions] is one of the seminal results in copula theory, which extended the applications of copulas, and explained why copulas captures the dependence by relating the joint distributions to univariate margins.\n\n::: {#thm-sklar}\n## Fréchet-Hoeffding Bounds\n[@ecmr]\n\n1. Let $H$ be a joint distribution function with univariate margins $F$ and $G$. Then there exists a copula $C$ such that $\\forall x,y \\in \\mathbb{R}, H(x,y) = C(F(x), G(y))$. Furthermore, $C$ is **unique** in the case when $F, G$ are continuous; otherwise, in the general case, $C$ is uniquely determined on $\\text{Ran}F \\times \\text{Ran}G$, where $\\text{Ran}F, \\text{Ran}G$ denote the ranges of $F, G$ respectively. That copula $C$ is given by: $C(u,v)=H(F^{\\leftarrow}(u), G^{\\leftarrow}(u))$ such that $(u,v) \\in \\text{Ran}F \\times \\text{Ran}G$.\n\n2. Conversely, $H$ is defined as a 2-D distribution function with marginals $F, G$, if we are given copula $C$ along with the univariate marginals $F, G$.\n:::\n\nIn this work, we will mainly deal with two dimensions, but @thm-sklar above can be generalized to n-D case as detailed in section 2.10 of @nelsen. Below, we include a few insights drawn from [@ecmr] that will be important to our ongoing discussion:\n\n:::{.callout-note}\n@thm-sklar gives us an insight into the name copula as in how it \"couples\" a joint distribution function to its marginal distributions. This coupling effect and two parts of @thm-sklar show us how we can separate (or combine) multivariate dependence structure and univariate margins.\n:::\n\n:::{.callout-warning}\n## Spoiler Alert\nIn the case of continuous random variables, there is only one **unique** copula that characterizes the multivariate dependence structure, which is very convenient for reasons we will discuss later in this chapter. This is not the case with discrete variables, which make the direct use of continuous copulas intractable.\n:::\n\n:::{.callout-note}\n@thm-sklar can be used to verify the existence of a continuous distribution function $H$ in case of a multivariate dataset if and only if we are sure of the existence of corresponding continuous univariate marginals for each variable in the dataset.\n:::\n\n## The Invariance Principle\n\nAs we saw in the motivating example, the underlying dependence structure did not change over a certain type of transformations. This was very convenient for us, and thus is a favorable property for a copula to have. This property is often formally referred to as \"invariance\", which we will formalize in the following theorem from [@ecmr]\n\n::: {#thm-invariance}\n## Invariance Principle\nLet $(X, Y) \\sim H$ with continuous margins $F, G$ and copula $C$. If $T_X, T_Y$ are **strictly increasing** transformations on $\\text{Ran}X, \\text{Ran}Y$, respectively, then $(T_X(X), T_Y(Y))$ also has copula $C$.\n:::\n\n:::{.callout-note}\n@thm-invariance was implicitly in action during our analysis for the motivating example because the transformations that we used were of two kinds, namely, probability integral transformation and quantile transformation, and in both of the cases, we were dealing with continuous and **strictly increasing** mappings on the respective ranges of random variables.\n:::\n\n## Measures of Association and Copula Estimation\n\nNow that we have built an object (copula) that allows us to just capture the multivariate dependence structure between variables, we would like to encode certain pieces of this information into a set of robust measures or metrics. We would call these measures, the **measures of association**. There are two types of measures of association: parametric and non-parametric. As discussed briefly for our motivating example, a common (parametric) measure of association is the Pearson correlation coefficient ($\\rho_{pearson}$). Although it is really efficient to calculate, it only captures linear dependence between the random data vectors at hand. Let's discuss this metric in more detail along with its limitations:\n\n### Pearson's Correlation Coefficient ($\\rho_{pearson}$) & its Properties\n\n::: {#def-pearson}\n### Pearson correlation coefficient\nGiven a random vector $(X, Y)$ with $Var(X) < \\infty$ and $Var(Y) < \\infty$, then:\n\n$$\n\\rho_{pearson}(X, Y) = \\frac{Cov(X, Y)}{\\sqrt{Var(X)}\\sqrt{Var(Y)}}\n$$\n, where covariance is defined as: \n$$\nCov(X, Y) = \\mathbb{E}((X - \\mathbb{E}(X))(Y - \\mathbb{E}(Y)))\n$$\n, and the variance is defined as $Var(X) = \\mathbb{E}((X - \\mathbb{E}(X))^2)$.\n:::\n\nLet's start by going over some commonly-used properties of $\\rho_{pearson}$ as mentioned in @ecmr:\n\n1. $\\rho_{pearson} \\in [-1, 1]$\n\n2. $|\\rho_{pearson}(X, Y)|$ = 1 if and only if $\\exists a,b \\in \\mathbb{R}$, with $a \\neq 0$ such that $Y = aX + b$ almost surely with $a < 0$ if and only if $\\rho_{pearson}(X, Y) = -1$, and $a > 0$ if and only if $\\rho_{pearson}(X, Y) = 1$. In both cases, $X, Y$ are called _perfectly linearly dependent_\n\n3. If $X$ and $Y$ ar independent, then $\\rho_{pearson}(X, Y) = 0$.\n\n4. $\\rho_{pearson}$ is invariant under _strictly increasing linear_ transformations.\n\n### Limitations of Pearson's Correlation Coefficient ($\\rho_{pearson}$)\n\nAlthough Pearson’s correlation coefficient $\\rho_{\\text{pearson}}$ is useful in many cases, it only captures **linear dependence** and ignores non-linear relationships. Below, we summarize its key limitations along with illustrative examples.\n\n1. **Non-Existence of $\\rho_{pearson}$:** Pearson's correlation does not exist for every random vector $(X, Y)$, particularly when variances (or other higher order moments) are undefined.\n\n:::{.callout-note}\n## Example: Heavy-Tailed Distributions\nConsider two independent random variables $X_1, X_2$ drawn from a **Pareto(3)** distribution with $F(x) = 1 - x^{-3}, \\quad x \\geq 1$. Define $X = X_1$, and $Y = X_1^2$. The covariance is given by $Cov(X, Y) = Cov(X_1, X_1^2) = \\mathbb{E}(X_1^3) - \\mathbb{E}(X_1)\\mathbb{E}(X_1^2)$. For Pareto(3), it is well-known (and can be easily proven) that $\\mathbb{E}(X_1^3)$ **does not exist** (as the integral diverges). Since Pearson's formula rely on this moment, $\\rho_{pearson}(X, Y)$ **doesn't exist**. On the other hand, we can observe that $Y = X^2$ shows a **perfect functional dependence**, since $Y$ can be represented as a deterministic (quadratic) function of $X$.\n:::\n\n2. **Non-Invariance Under Non-Linear Transformations:** $\\rho_{pearson}$ is not necessarily invariant under all strictly increasing transformations on $\\text{Ran}X$ or $\\text{Ran}Y$.\n\n:::{.callout-note}\n## Example: Logarithmic Transformation on $U(0, 1)$\nLet $X \\sim U(0,1)$ and define $Y = \\log(X)$. Pearson’s correlation is: $\\rho_{\\text{pearson}}(X, Y) = \\frac{\\text{Cov}(X, \\log X)}{\\sigma_X \\sigma_Y}$. Even though$Y = \\log(X)$ is a **strictly increasing function**, $\\rho_{pearson}$ changes under this transformation. Thus, Pearson's correlation is **not invariant** under (non-linear) monotonic transformations such as $\\log$ in certain situations.\n:::\n\n3. **Uncorrelatedness Does Not Imply Independence:** $\\rho_{pearson} = 0$ does NOT necessarily imply that $(X, Y)$ are independent.\n\n:::{.callout-note}\n## Example: Quadratic Transformation on $U(-1, 1)$\nLet $X \\sim U(-1,1)$ and define: $Y = X^2$. We can compute: $\\mathbb{E}[X] = 0, \\quad \\mathbb{E}[Y] = \\mathbb{E}[X^2] = \\frac{1}{3}$. Now, consider the covariance: $\\text{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X] \\mathbb{E}[Y] = \\mathbb{E}[X^3] - (0)(\\frac{1}{3})$. Since $\\mathbb{E}[X^3] = 0$, we get $\\text{Cov}(X, Y) = 0$. Thus, $\\rho_{\\text{pearson}}(X, Y) = 0$, but **$X$ and $Y$ are clearly dependent**, since knowing $X$ exactly determines $Y$. This example demonstrates that a zero Pearson correlation does **not** imply statistical independence.\n:::\n\n4. **Non-Uniqueness of the Joint Distribution Given Marginals and $\\rho_{pearson}$:** The marginal distributions and the correlation coefficient do not uniquely determine the joint distribution. \n\n:::{.callout-note}\n## Example: Bivariate Normal and Mixture Distributions\nConsider two bivariate distributions:\n\n1. **Bivariate Normal Distribution**:  \n   $$\n   (X_1, X_2) \\sim N \\left( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix} \\right).\n   $$\n\n2. **Bivariate Mixture Distribution** (Same Marginals, Different Dependence):  \n   $$\n   X_1 \\sim N(0,1), \\quad X_2 = \\begin{cases} \n   X_1, & \\text{with probability } 0.75, \\\\\n   -X_1, & \\text{with probability } 0.25.\n   \\end{cases}\n   $$\n\nBoth cases yield: $\\rho_{pearson}(X_1, X_2) = 0.5$.\n\nHowever, their **joint distributions are completely different**, meaning **$\\rho_{pearson}$ does not uniquely determine dependence**.\n:::\n\n5. **Unattainability of Certain Correlations:** Given margins $F_1, F_2$, some $\\rho_{pearson} \\in [-1,1]$ values cannot be attained by choosing any possible copula for $(X_1, X_2)$. An example demonstrating this can be found in @ecmr p.46\n\nIn order to circumvent some of the limitations of pearson coefficient, we now consider rank-based correlation measures such as Spearman's Rho ($\\rho_{spearman}$) and Kendall's Tau ($\\tau_{kendall}$) as they only depend on the underlying copula $C$ at least in the case of continuous random variables. Again, we will discuss the pecularities of the discrete case later in this chapter.\n\nThese rank-based measures are also known as **measures of concordance**. [@ecmr] In order to better understand this, we would first need to define _concordance_. Consider two points in $\\mathbb{R}^2$, $(x_1, y_1)$ and $(x_2, y_2)$. These points are defined as concordant if $(x_1 - x_2)(y_1 - y_2) > 0$ and discordant if $(x_1 - x_2)(y_1 - y_2) < 0$.\n\n### Kendall's Tau\n\n::: {#def-kendalls-tau}\n### Kendall's Tau\nGiven a bivariate random vector $(X_1, X_2)$ with continuous marginals $F_1$ and $F_2$, let's define $(X_1', X_2')$ as an independent copy of $(X_1, X_2)$. Then the population version of Kendall's tau is defined by:\n\n$$\n\\tau_{kendall}(X_1, X_2) = \\mathbb{E}(\\text{sign}((X_1 - X_1')(X_2 - X_2')))\n$$\nHere, $\\text{sign}(x)$ is the sign-function defined in a piecewise manner as follows:\n$$\n\\text{sign}(x) = \\begin{cases} \n   -1, & \\text{if } x < 0, \\\\\n   0, & \\text{if } x = 0, \\\\\n   1, & \\text{if } x > 0.\n   \\end{cases}\n$$\n:::\n\nUsing the above-mentioned notion of concordance, definition of an expected value, and @def-kendalls-tau, we can equivalently define Kendall's Tau as $\\tau_{kendall} = (1)\\mathbb{P}((X_1 - X_1')(X_2 - X_2') > 0) + (0)\\mathbb{P}((X_1 - X_1')(X_2 - X_2') = 0) + (-1)\\mathbb{P}((X_1 - X_1')(X_2 - X_2') < 0) = \\mathbb{P}((X_1 - X_1')(X_2 - X_2') > 0) - \\mathbb{P}((X_1 - X_1')(X_2 - X_2') < 0)$, since in the case of continuous distributions, probability at any given point is 0, specifically $\\mathbb{P}((X_1 - X_1')(X_2 - X_2') = 0) = 0$. \n\nAs mentioned in @ecmr p.53, we can represent $\\tau_{kendall}$ in terms of an underlying copula $C$ as $\\tau_{kendall}(C) = 4 \\int_{[0, 1]^2}C(u,v)d(C(u,v)) - 1$.  \n\n### Spearman's Rho \n\n::: {#def-spearman}\n### Spearman's Rho\nGiven a bivariate random vector $(X_1, X_2)$ with continuous marginals $F_1$ and $F_2$, then the population version of Spearman's rho is defined by:\n\n$$\n\\rho_{spearman}(X_1, X_2) = \\rho_{pearson}(F_1(X_1), F_2(X_2))\n$$\nWe can observe that the Spearman's rho is nothing but Pearson's correlation coefficient of the transformed variables obtained after performing the Probability Integral Transformation defined earlier in @lem-prob-int-trans.\n:::\n\nAs mentioned in @ecmr p.53, we can represent $\\rho_{spearman}$ in terms of an underlying copula $C$ as $\\rho_{spearman}(C) = 12 \\int_{[0, 1]^2}C(u,v)d((u,v)) - 3$.\n\n\n:::{.callout-note}\n## Note:\n$\\tau_{kendall}$ and $\\rho_{spearman}$ both overcome the significant limitations of $\\rho_{pearson}$ with the following properties as summarized in @ecmr:\n\n* These measures always exist, and are invariance under all (not just linear) strictly increasing tranformations\n\n* These measures attain all values in $[-1, 1]$, and they specifically attain -1 and 1 when the copula $C$ attains the Fréchet-Hoeffding bounds $W$ and $M$ as defined in @thm-fh-bounds\n\n:::\n\n## Does Everything Work in the Discrete Case as Well?\n\nUp to this point, our discussion has centered on continuous random variables. Many of the results and definitions we have used rely on continuity, which ensures that the probability integral transform (PIT) maps each variable to a uniform distribution on $[0,1]$. This property, in turn, guarantees the uniqueness of the copula associated with a joint distribution via Sklar’s theorem. In our earlier work, we have taken this uniqueness for granted.\n\nHowever, real-world data are often **discrete**. When dealing with discrete random variables, the marginal distribution functions are not continuous, and the PIT no longer produces uniform random variables on the full interval $[0,1]$. Instead, we obtain what is known as a **subcopula**—a function defined only on a proper subset of $[0,1]^2$, namely on the ranges of the marginal distributions.\n\n::: {.callout-note}\n## Example: Bivariate Bernoulli Distribution\n\n*Imagine a bivariate distribution where each variable follows a Bernoulli law. In this setting, the only possible values for each variable are 0 and 1. The resulting subcopula is then defined on the set of points. Because this set is a proper subset of $[0,1]^2$, the corresponding copula is not uniquely determined by the joint distribution of the variables.*\n\n:::\n\n### Unidentifiability Issue\n\nNow, let us examine the unidentifiability problem in more detail. To illustrate the issue, consider the following adapted example in the two-dimensional case, inspired by @Geenens. Suppose we have a subcopula $C^S$ defined on a discrete domain, where $D_1=\\operatorname{Ran}(F)$ and $D_2=\\operatorname{Ran}(G)$ with the marginal distribution functions $F$ and $G$, respectively. In the continuous case, a two-dimensional (sub)copula is defined on the entire unit square $[0,1]^2$. By contrast, for discrete random variables, the subcopula $C^S$ is only uniquely specified on the domain $D_1 \\times D_2 = \\operatorname{Ran}(F) \\times \\operatorname{Ran}(G)$.\n\nTo obtain a full copula $C$ on $[0,1]^2$, one must “fill in” the gaps—that is, extend the definition of $C^S$ to those parts of the unit square not covered by $D_1 \\times D_2$. Unfortunately, there are uncountably many ways to perform this extension while still satisfying the fundamental properties required of a copula in its @def-2d-copula. This leads to a **non-uniqueness** (or **unidentifiability**) issue, which complicates both the development and the application of copula-based models for discrete data. This unidentifiability has been examined in depth in the literature such as @Geenens, and it calls into question the straightforward (direct) application of copula methods when at least one margin is discrete.\n\nOne of the ways to fill in the gaps is by performaing a Distributional Transform, which basically serves to add random \"noise\" to each of the gaps in parent distribution as described by @RUSCHENDORF and @Faugeras. Formally, considering a random variable $X \\sim F$ and independently, consider $V \\sim U(0,1)$, then the distributional transform of $X$ is $F(X, V) = P(X < x) + V * P(X = x)$. After applying this, we can directly proceed to apply results from continuous copula modeling as we have smoothened out the discontinuities. Another method that also accomplishes this goal is described in the next chapter.\n\n\n\n### Margin-Dependence of Concordance Association Measures\n\nXXX\n\n\n\n\n\n<!-- \nThe following section should be included at the end of each chapter that contains code.\nNote that this will include code from code chunks even if `eval` is set to `false`.\n-->\n\n## @sec-simple Code\n\nThe following code was used to create @sec-simple.\n\n### Code within chapter\n```{r}\n#| echo: true\n#| eval: false\n#| ref-label: !expr knitr::all_labels()\n```\n\n### Code sourced from external scripts\n```{r}\n#| echo: true\n#| eval: false\n{{< include src/temp01-clean-wnba.R >}}\n```\n","srcMarkdownNoYaml":""},"formats":{"amherst-thesis-pdf":{"identifier":{"display-name":"PDF","target-format":"amherst-thesis-pdf","base-format":"pdf","extension-name":"amherst-thesis"},"execute":{"fig-width":3.6,"fig-height":2.4,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"shortcodes":[]},"pandoc":{"pdf-engine":"pdflatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","toc-depth":2,"number-sections":true,"highlight-style":"atom-one","filters":[],"output-file":"02-ch2-dep-copula.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":false,"number-depth":2,"knitr":{"opts_chunk":{"echo":false,"warning":false,"message":false}},"documentclass":"report","classoption":"twoside","fontsize":"11pt","geometry":["inner=1.5in","outer=1in","top=1in","bottom=1in","heightrounded"],"colorlinks":true,"linestretch":2,"code-block-bg":"#f8f8f8","code-block-border-left":true,"template-partials":["_extensions/Amherst-Statistics/amherst-thesis/before-title.tex","_extensions/Amherst-Statistics/amherst-thesis/title.tex"],"revealjs-plugins":[],"bibliography":["references.bib"],"csl":"includes/american-statistical-association.csl","department":"Mathematics and Statistics","advisors":"Professor Shu-Min Liao","submitted":2025},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["amherst-thesis-pdf"]}