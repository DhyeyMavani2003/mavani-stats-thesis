{
  "hash": "ca04aeb15bbcbc89b45b3e3d4ae34057",
  "result": {
    "engine": "knitr",
    "markdown": "# Unraveling Notion of Dependence through Copulas {#sec-simple}\n\n\n\n\n\n<!-- Make it accessible by starting with a motivating example, then proceed towards more rigorousness gradually until challenges for discrete case are covered. -->\n\nIn this chapter, we will formalize some aspects of phenomena such as dependence and association. In order to aid us in our understanding, we will use two bivariate random vectors. Along the way, we will visualize and analyze various aspects of their dependence and association through Python code.\n\nConsider $(X_1, X_2)$ and $(Y_1, Y_2)$ be bivariate random vectors, each consisting of 10000 independent data-points, which are distributed with the joint distributions $F_X$ and $F_Y$ respectively. Given these bivariate vectors, one might ask: How can I compare the relationship between $(X_1,X_2)$ to the relationship between $(Y_1, Y_2)$? One of the measures that can help us compare and contrast these relationships is Pearson correlation coefficient (commonly denoted as $\\rho_{pearson}$). After preliminary calculations on a Python3 kernel, we can see that $\\rho_{pearson}(X_1, X_2) \\approx 0.802$, but on the other hand, the correlation between $\\rho_{pearson}(Y_1, Y_2) \\approx 0.755$. From these measure-values, it seems that the dependence between $(X_1,X_2)$ is stronger than the dependence between $(Y_1, Y_2)$. Although this agrees with our scatter plots in @fig-motivating-example, it is vital to note that $\\rho_{pearson}$ only captures the linear dependence between the underlying random variables at hand.\n\nUpon observing the @fig-motivating-example closely, we note that the marginal distributions of $X_1$ and $X_2$ are close to normal, unlike the marginals of $Y_1$ and $Y_2$. Moreover, we can see that the relationship between $Y_1$ and $Y_2$ is non-linear. This vast difference in marginals takes away our trust from the appropriateness of the use of $\\rho_{pearson}$ as a measure to compare dependence between the data vectors at hand.\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatter plots of 10000 independent observations of $(X_1, X_2)$ and $(Y_1, Y_2)$ with overlaid curves depicting respective marginal distributions.](fig/motivating_example.png){#fig-motivating-example width=5.9in}\n:::\n:::\n\n\nLet's introduce a lemma that will help us transform the marginals so that the resulting marginals are more similar, and try to only capture or extract the \"dependence\" components, which will allow us to make fairer comparisons. \n\n::: {#lem-prob-int-trans}\n## Probability Integral Transformation\n[@ecmr]\nLet $F$ be a continuous distribution function and let $X \\sim F$, then $F(X)$ is a standard uniform random variable, that is, $F(X) \\sim U(0,1)$.\n:::\n\n@lem-prob-int-trans allows us to transform a continuous random variable to a random variable which has standard uniform distribution. So, by using this transformation, we can now convert our marginals $X_1, X_2, Y_1, Y_2$ individually to be distributed $\\text{Uniform}(0,1)$. And, since now the resulting marginals will all be of the same type, it will allow us to compare the dependence between random variables on fairer grounds.\n\nFor instance, if we know that $X_1 \\sim N(0,1) = F_1$, $X_2 \\sim N(0,1) = F_2$, $Y_1 \\sim Gamma(3, 15) = G_1$, and $Y_2 \\sim Beta(5, 3) = G_2$, where $F_1, F_2, G_1, G_2$ denote the distribution functions of the respective random variables. By @lem-prob-int-trans, we can say that $F_1(X_1), F_2(X_2), G_1(Y_1),$ and $G_2(Y_2)$ are each distributed $\\text{Uniform}(0,1)$.\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatter plots of 10000 independent observations of ($F_1(X_1)$, $F_2(X_2)$) and ($G_1(Y_1)$, $G_2(Y_2)$) with overlaid curves depicting respective marginal distributions.](fig/transformed_motivating_example.png){#fig-transformed-motivating-example width=6.01in}\n:::\n:::\n\n\nLooking at @fig-transformed-motivating-example, we can see that the transformed data vectors appear to be significantly similar. We can computationally verify this by quickly calculating the $\\rho_{pearson}$ for ($F_1(X_1)$, $F_2(X_2)$) and ($G_1(Y_1)$, $G_2(Y_2)$), which turns out to be 0.788 for both data vector pairs, implying that both have same dependence.\n\n\n## Tables\n\nYour tables should be publication quality. Consider using [**gt**](https://gt.rstudio.com/articles/gt.html) [@gt] or  [**kableExtra**](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_pdf.pdf) [@kableExtra] to customize your tables. The [**gtsummary**](https://www.danieldsjoberg.com/gtsummary/) package [@gtsummary] may also come in handy.\n\n<!-- \nThe following section should be included at the end of each chapter that contains code.\nNote that this will include code from code chunks even if `eval` is set to `false`.\n-->\n\n## @sec-simple Code\n\nThe following code was used to create @sec-simple.\n\n### Code within chapter\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load knitr package\nlibrary(knitr)\n\n# Python Engine Setup\nknit_engines$set(python3 = knit_engines$get(\"python\"))\n\n# Load packages\nlibrary(tidyverse)\nlibrary(gt)\n\n# Set default ggplot theme for document\ntheme_set(theme_classic())\n# If using kableExtra tables, print blank cells instead of `NA`\noptions(knitr.kable.NA = \"\")\n\n# Load NBA Data\nload(\"data/temp_wnba.RData\")\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta, expon, norm, gamma, binom\nimport os\n\n# Create directory if not exists\nfig_dir = \"fig\"\nos.makedirs(fig_dir, exist_ok=True)\n\n# Generate Data\nnp.random.seed(8990)\nn = 10000\nmean = [0, 0]\ncov = [[1, 0.8], [0.8, 1]]\nX = np.random.multivariate_normal(mean, cov, size=n)\nX1, X2 = X[:, 0], X[:, 1]\n\n# Transform X1 and X2 to uniform [0, 1] using the CDF of the normal distribution\nU1 = norm.cdf(X1)\nU2 = norm.cdf(X2)\n\n# Transform U1 and U2 into Gamma and Beta distributions\nY1 = gamma.ppf(U1, 3, 15)\nY2 = beta.ppf(U2, 5, 3)\n\n# Calculate Pearson Correlation Coefficients\nrho_X = np.corrcoef(X1, X2)[0, 1]\nrho_Y = np.corrcoef(Y1, Y2)[0, 1]\nprint(\"Pearson correlation for (X1, X2):\", rho_X)\nprint(\"Pearson correlation for (Y1, Y2):\", rho_Y)\n\n# Create Layout design and Set Size-Ratio\nfig, axes = plt.subplots(1, 2, figsize=(6, 4))\n\n# Scatter plot for (X1, X2)\naxes[0].scatter(X1, X2, alpha=0.3, s=5)\naxes[0].set_title(\"Scatter plot of (X1, X2)\")\naxes[0].set_xlabel(\"X1\")\naxes[0].set_ylabel(\"X2\")\n\n# Add marginal histograms\naxes[0].hist(X1, bins=50, density=True, alpha=0.9, color='blue', orientation='vertical', histtype='step')\naxes[0].hist(X2, bins=50, density=True, alpha=0.9, color='red', histtype='step', orientation='horizontal')\n\n# Scatter plot for (Y1, Y2)\naxes[1].scatter(Y1, Y2, alpha=0.3, s=5)\naxes[1].set_title(\"Scatter plot of (Y1, Y2)\")\naxes[1].set_xlabel(\"Y1\")\naxes[1].set_ylabel(\"Y2\")\n\n# Add marginal histograms\naxes[1].hist(Y1, bins=50, density=True, alpha=0.9, color='blue', orientation='vertical', histtype='step')\naxes[1].hist(Y2, bins=50, density=True, alpha=0.9, color='red', histtype='step', orientation='horizontal')\n\n# Organize into a tight layout as per matplotlib\nplt.tight_layout()\n\n# Save figure instead of showing it\nfig_path = os.path.join(fig_dir, \"motivating_example.png\")\nplt.savefig(fig_path, dpi=300, bbox_inches='tight')\n\n# Close the figure to prevent rendering output\nplt.close(fig)  \nknitr::include_graphics(\"fig/motivating_example.png\")\n\n# Set random seed for reproducibility\nnp.random.seed(8990)\n\n# Number of samples\nn = 10000\n\n# Generate bivariate normal data (X1, X2)\nmean = [0, 0]\ncov = [[1, 0.8], [0.8, 1]]  # Correlation ~ 0.8\nX = np.random.multivariate_normal(mean, cov, size=n)\nX1, X2 = X[:, 0], X[:, 1]\n\n# Transform X1 and X2 to uniform [0, 1] using normal CDF\nU_X1 = norm.cdf(X1)\nU_X2 = norm.cdf(X2)\n\n# Transform uniform marginals to desired distributions\nY1 = gamma.ppf(U_X1, a=3, scale=1/15) \nY2 = beta.ppf(U_X2, a=5, b=3)\n\n# Apply probability integral transformation to all variables to make them uniform\nU_Y1 = gamma.cdf(Y1, a=3, scale=1/15)\nU_Y2 = beta.cdf(Y2, a=5, b=3)\n\n# Calculate Pearson Correlation Coefficients\nrho_U_X = np.corrcoef(U_X1, U_X2)[0, 1]\nrho_U_Y = np.corrcoef(U_Y1, U_Y2)[0, 1]\nprint(\"Pearson correlation for ($F_1(X_1)$, $F_2(X_2)$):\", rho_U_X)\nprint(\"Pearson correlation for ($G_1(Y_1)$, $G_2(Y_2)$):\", rho_U_Y)\n\n# Combine transformed data\nuniform_data = np.vstack([U_X1, U_X2, U_Y1, U_Y2]).T\n\n# Verify the uniformity of transformed data (Should be 0.5 in value)\nprint(\"U_X1 mean:\", U_X1.mean(), \"U_X2 mean:\", U_X2.mean())\nprint(\"U_Y1 mean:\", U_Y1.mean(), \"U_Y2 mean:\", U_Y2.mean())\n\n# Create Layout design and Set Size-Ratio\nfig, axes = plt.subplots(1, 2, figsize=(6, 4))\n\n# Scatter plot for (U_X1, U_X2)\naxes[0].scatter(U_X1, U_X2, alpha=0.3, s=5)\naxes[0].set_title(\"Scatter plot of ($F_1(X_1)$, $F_2(X_2)$)\")\naxes[0].set_xlabel(\"$F_1(X_1)$\")\naxes[0].set_ylabel(\"$F_2(X_2)$\")\n\n# Add marginal histograms\naxes[0].hist(U_X1, bins=50, density=True, alpha=0.9, color='blue', orientation='vertical', histtype='step')\naxes[0].hist(U_X2, bins=50, density=True, alpha=0.9, color='red', histtype='step', orientation='horizontal')\n\n# Scatter plot for (U_Y1, U_Y2)\naxes[1].scatter(U_Y1, U_Y2, alpha=0.3, s=5)\naxes[1].set_title(\"Scatter plot of ($G_1(Y_1)$, $G_2(Y_2)$)\")\naxes[1].set_xlabel(\"$G_1(Y_1)$\")\naxes[1].set_ylabel(\"$G_2(Y_2)$\")\n\n# Add marginal histograms\naxes[1].hist(U_Y1, bins=50, density=True, alpha=0.9, color='blue', orientation='vertical', histtype='step')\naxes[1].hist(U_Y2, bins=50, density=True, alpha=0.9, color='red', histtype='step', orientation='horizontal')\n\n# Organize into a tight layout as per matplotlib\nplt.tight_layout()\n\n# Save figure instead of showing it\nfig_path = os.path.join(fig_dir, \"transformed_motivating_example.png\")\nplt.savefig(fig_path, dpi=300, bbox_inches='tight')\n\n# Close the figure to prevent rendering output\nplt.close(fig)  \nknitr::include_graphics(\"fig/transformed_motivating_example.png\")\n\n# =======================================================================\n# Sample R script for thesis template\n#\n# Cleans temp_raw_wnba.csv dataset, which contains data pulled from\n# https://www.espn.com/wnba/stats/player on 2024/06/19\n#\n# Last updated: 2024/06/19\n# =======================================================================\nlibrary(tidyverse)\n\nwnba <- read_csv(\"data/temp_raw_wnba.csv\") |> \n  janitor::clean_names() |> \n  # Pull jersey numbers off of names and \n  # turn height text into msmt (6'4\" = 6.3333)\n  mutate(jersey = str_extract(name, \"[0-9]+$\"),\n         name = str_remove(name, \"[0-9]+$\"),\n         ht_ft = parse_number(str_extract(ht, \"^[0-9]\")),\n         ht_in = parse_number(str_extract(ht, '[0-9]+\\\\\"$')),\n         height = ht_ft * 12 + ht_in,\n         weight = parse_number(wt),\n         position = factor(pos,\n                           levels = c(\"G\", \"F\", \"C\"),\n                           labels = c(\"Guard\", \"Forward\", \"Center\"))) |> \n  select(-c(ht, wt, ht_ft, ht_in, pos))\n  \nsave(wnba, file = \"data/temp_wnba.RData\")\n```\n:::\n\n\n### Code sourced from external scripts\n\n::: {.cell}\n\n```{.r .cell-code}\n# =======================================================================\n# Sample R script for thesis template\n#\n# Cleans temp_raw_wnba.csv dataset, which contains data pulled from\n# https://www.espn.com/wnba/stats/player on 2024/06/19\n#\n# Last updated: 2024/06/19\n# =======================================================================\nlibrary(tidyverse)\n\nwnba <- read_csv(\"data/temp_raw_wnba.csv\") |> \n  janitor::clean_names() |> \n  # Pull jersey numbers off of names and \n  # turn height text into msmt (6'4\" = 6.3333)\n  mutate(jersey = str_extract(name, \"[0-9]+$\"),\n         name = str_remove(name, \"[0-9]+$\"),\n         ht_ft = parse_number(str_extract(ht, \"^[0-9]\")),\n         ht_in = parse_number(str_extract(ht, '[0-9]+\\\\\"$')),\n         height = ht_ft * 12 + ht_in,\n         weight = parse_number(wt),\n         position = factor(pos,\n                           levels = c(\"G\", \"F\", \"C\"),\n                           labels = c(\"Guard\", \"Forward\", \"Center\"))) |> \n  select(-c(ht, wt, ht_ft, ht_in, pos))\n  \nsave(wnba, file = \"data/temp_wnba.RData\")\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}