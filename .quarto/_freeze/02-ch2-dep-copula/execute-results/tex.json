{
  "hash": "205224332e693f6209d3f51ed128e9ab",
  "result": {
    "engine": "knitr",
    "markdown": "# Unraveling Notion of Dependence through Copulas {#sec-simple}\n\n\n\n\n\n<!-- Make it accessible by starting with a motivating example, then proceed towards more rigorousness gradually until challenges for discrete case are covered. -->\n\nIn this chapter, we aim to formalize concepts of dependence and association. To facilitate our understanding, we will use two bivariate random vectors and visualize their relationships through Python code.\n\n## Motivating Example\n\nConsider $(X_1, X_2)$ and $(Y_1, Y_2)$ be bivariate random vectors, each consisting of 10000 independent data-points, which are distributed with the joint distributions $F_X$ and $F_Y$ respectively. Given these bivariate vectors, one might ask: How can I compare the relationship between $(X_1,X_2)$ to the relationship between $(Y_1, Y_2)$? One of the measures that can help us compare and contrast these relationships is Pearson correlation coefficient (commonly denoted as $\\rho_{pearson}$). After preliminary calculations on a Python3 kernel, we can see that $\\rho_{pearson}(X_1, X_2) \\approx 0.802$, but on the other hand, the correlation between $\\rho_{pearson}(Y_1, Y_2) \\approx 0.755$. From these measure-values, it seems that the dependence between $(X_1,X_2)$ is stronger than the dependence between $(Y_1, Y_2)$. Although this agrees with our scatter plots in @fig-motivating-example, it is vital to note that $\\rho_{pearson}$ only captures the linear dependence between the underlying random variables at hand.\n\nUpon observing the @fig-motivating-example closely, we note that the marginal distributions of $X_1$ and $X_2$ are close to normal, unlike the marginals of $Y_1$ and $Y_2$. Moreover, we can see that the relationship between $Y_1$ and $Y_2$ is non-linear. This vast difference in marginals takes away our trust from the appropriateness of the use of $\\rho_{pearson}$ as a measure to compare dependence between the data vectors at hand.\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatter plots of 10000 independent observations of $(X_1, X_2)$ and $(Y_1, Y_2)$ with overlaid curves depicting respective marginal distributions.](fig/motivating_example.png){#fig-motivating-example width=5.9in}\n:::\n:::\n\n\nLet's introduce a lemma that will help us transform the marginals so that the resulting marginals are more similar, and try to only capture or extract the \"dependence\" components, which will allow us to make fairer comparisons. \n\n::: {#lem-prob-int-trans}\n## Probability Integral Transformation\n[@ecmr]\nLet $F$ be a continuous distribution function and let $X \\sim F$, then $F(X)$ is a standard uniform random variable, that is, $F(X) \\sim U(0,1)$.\n:::\n\n@lem-prob-int-trans allows us to transform a continuous random variable to a random variable which has standard uniform distribution. So, by using this transformation, we can now convert our marginals $X_1, X_2, Y_1, Y_2$ individually to be distributed $\\text{Uniform}(0,1)$. And, since now the resulting marginals will all be of the same type, it will allow us to compare the dependence between random variables on fairer grounds.\n\nFor instance, if we know that $X_1 \\sim N(0,1) = F_1$, $X_2 \\sim N(0,1) = F_2$, $Y_1 \\sim Gamma(3, 15) = G_1$, and $Y_2 \\sim Beta(5, 3) = G_2$, where $F_1, F_2, G_1, G_2$ denote the distribution functions of the respective random variables. By @lem-prob-int-trans, we can say that $F_1(X_1), F_2(X_2), G_1(Y_1),$ and $G_2(Y_2)$ are each distributed $\\text{Uniform}(0,1)$.\n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatter plots of 10000 independent observations of ($F_1(X_1)$, $F_2(X_2)$) and ($G_1(Y_1)$, $G_2(Y_2)$) with overlaid curves depicting respective marginal distributions.](fig/transformed_motivating_example.png){#fig-transformed-motivating-example width=6.01in}\n:::\n:::\n\n\nLooking at @fig-transformed-motivating-example, we can see that the transformed data vectors appear to be significantly similar. We can computationally verify this by quickly calculating the $\\rho_{pearson}$ for ($F_1(X_1)$, $F_2(X_2)$) and ($G_1(Y_1)$, $G_2(Y_2)$), which turns out to be 0.788 for both data vector pairs, meaning that both have same dependence structures.\n\nAn alternative way to approach the problem (of comparing dependence of distinct pairs of marginals), is by transforming the marginals of $(Y_1, Y_2)$ to be normal (same as marginals of $(X_1, X_2)$). As one can predict, in order to accomplish this transformation, we would need to \"undo the current distributional mappings on $(Y_1, Y_2)$\", which we can formally define as generalized inverse as follows:\n\n::: {#def-quantile-function}\n## Quantile Function\n[@ecmr]\n$F^{\\leftarrow}$ (Quantile Function) is defined as $F^{\\leftarrow}(y) = \\text{inf} \\{x \\in \\mathbb{R} | F(x) \\geq y \\}$, where $y \\in [0, 1]$, and $\\text{inf}$ is the infimum of a set.\n:::\n\n:::{.callout-warning}\nThe quantile function $F^{\\leftarrow} = F^{-1}$ only when $F$ is continuous and strictly increasing. Thus it is important to note that, in other cases, the ordinary inverse $F^{-1}$ need not exist. [@ecmr]\n:::\n\nWith the above definition of $F^{\\leftarrow}$, let's introduce a lemma from [@ecmr] that will help us perform the transformation to normal.\n\n::: {#lem-quantile-transformation}\n## Quantile Transformation\n[@ecmr]\nLet $U \\sim \\text{Unif}(0, 1)$ and let $F$ be any distribution function be a distribution function. Then $F^{\\leftarrow}(U) \\sim F$, that is, $F^{\\leftarrow}(X)$ is distributed with density $F$.\n:::\n\n:::{.callout-note}\n@lem-quantile-transformation is valid for non-continuous densities $F$ as well. [@ecmr]\n:::\n\nLet's start with the transformations where we left off in @fig-transformed-motivating-example, since we have uniform densities there. Applying @lem-quantile-transformation on $G_1(Y_1)$ and $G_2(Y_2)$ using quantile functions $F_1^{\\leftarrow} = F_1^{-1}$ and $F_2^{\\leftarrow} = F_2^{-1}$ respectively gives us that $F_1^{-1}(G_1(Y_1)) \\sim F_1$ and $F_2^{-1}(G_2(Y_2)) \\sim F_2$. \n\n::: {.cell}\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatter plots of 10000 independent observations of ($X_1$, $X_2$) and ($F_1^{-1}(G_1(Y_1))$, $F_2^{-1}(G_2(Y_2))$) with overlaid curves depicting respective marginal distributions.](fig/quantile_transformed_motivating_example.png){#fig-quantile-transformed-motivating-example width=6.32in}\n:::\n:::\n\n\nNotice in @fig-quantile-transformed-motivating-example that the resulting transformed distribution through this alternative method resembles that of $(X_1, X_2)$. Hence, we can conclude that they have the same dependence. Furthermore, through a quick calculation, we can see that $\\rho_{pearson}(F_1^{-1}(G_1(Y_1)), F_2^{-1}(G_2(Y_2))) = 0.802$, which is the same as the Pearson correlation coefficient between $X_1$ and $X_2$. This is the level of flexibility that a combination of transformations presented in @lem-prob-int-trans and @lem-quantile-transformation can lend us.\n\n:::{.callout-note}\n\"$(X_1, X_2)$ and $(Y_1, Y_2)$ have the same dependence\" $\\iff$ \"$(X_1, X_2)$ and $(Y_1, Y_2)$ have the same copula\" [@ecmr]\n:::\n\n## Copulas: A Unified Framework for Dependence\n\nCopulas are a class of multivariate distribution functions with $Unif(0, 1)$ marginals. The motivating example in the previous section explains the usage of copulas as the structures capturing margin-independent dependence between random variables.\n\n:::{.callout-note}\nThe choice of $Unif(0, 1)$ as a post-transformation margin for the data at hand is  somewhat arbitrary although it does simplify further results. One can use modifications of @lem-prob-int-trans and @lem-quantile-transformation to define copulas with respect to any margin of choice without affecting the final conclusions about the dependence between the data at hand. [@ecmr]\n:::\n\nIn order to understand copulas better, for now, let's restrict ourselves to the 2-D (2-dimensional) case. Firstly, let's introduce the definition of a broader class of functions called subcopulas as a preliminary, which will help us mathematically define copulas as a special case. [@nelsen]\n\n::: {#def-2d-subcopula}\n## 2-Dimensional Subcopula\n[@Erdely2017]\nA **two-dimensional subcopula** (2-subcopula) is a function $C^S: D_1 \\times D_2 \\to [0, 1]$, where $\\{0, 1\\} \\subseteq D_i \\subseteq [0, 1]$ for $i \\in \\{1, 2\\}$ with the following conditions satisfied:\n\n* _Grounded:_ $C^S(u, 0) = 0 = C^S(0, v)$, $\\forall u \\in D_1, \\forall v \\in D_2$.\n\n* _Marginal Consistency:_ $\\forall u \\in D_1$ and $\\forall v \\in D_2$, $C^S(u, 1) = u$ and $C^S(1, v) = v$.\n\n* _2-increasing:_ $\\forall u_1,u_2 \\in D_1$ and $\\forall v_1, v_2 \\in D_2$ such that $u_1 \\leq u_2$ and $v_1 \\leq v_2$, $C^S(u_1, v_1) - C^S(u_2, v_1) + C^S(u_2, v_2) - C^S(u_1, v_2) \\geq 0$.\n:::\n\n::: {#def-2d-copula}\n## 2-Dimensional Copula\n[@Erdely2017]\nA **two-dimensional copula** (2-copula) is a function $C: [0, 1] \\times [0, 1] \\to [0, 1]$, with the following conditions satisfied:\n\n* _Grounded:_ $C(u, 0) = 0 = C(0, v)$, $\\forall u \\in [0, 1], \\forall v \\in [0, 1]$.\n\n* _Marginal Consistency:_ $\\forall u \\in [0, 1]$ and $\\forall v \\in [0, 1]$, $C(u, 1) = u$ and $C(1, v) = v$.\n\n* _2-increasing:_ $\\forall u_1,u_2 \\in [0, 1]$ and $\\forall v_1, v_2 \\in [0, 1]$ such that $u_1 \\leq u_2$ and $v_1 \\leq v_2$, $C(u_1, v_1) - C(u_2, v_1) + C(u_2, v_2) - C(u_1, v_2) \\geq 0$.\n:::\n\n:::{.callout-note}\nA 2-D copula is essentially a 2-subcopula with a full unit square as domain ($D_1 = D_2 = [0, 1]$). Furthermore, copula and subcopula are the same within a domain with continuous variables. Later in this chapter, we will discuss why this doesn't hold when one of the variables is discrete. \n:::\n\nIn this work, we will mainly deal with 2-D copulas and subcopulas, but the definitions above can be generalized to n-D case with some notable exceptions detailed (with proofs) in section 2.10 of @nelsen. Moreover, there are many different families of copulas bearing peculiar properties and corresponding margins, we are not covering them in detail since that is not the focus of this work, and a comprehensive summary of many of these families can be found in chapter 3 of @ecmr.\n\n## Fréchet-Hoeffding Bounds\n\nXXX\n\n## Sklar's Theorem and its Corollaries\n\nXXX\n\n## The Invariance Principle\n\nXXX\n\n## Measures of Association and Copula Estimation\n\nXXX\n\n::: {.callout-tip}\n## Note on Moving from Continuous to Discrete Case\n\nMain High-Level Idea\n:::\n\n## Does Everything Work in Discrete Case as well?\n\nXXX + Motivation from end\n\n### Unidentifiability Issue\n\nXXX\n\n### Margin-Dependence of Concordance Association Measures\n\nXXX\n\n\n\n\n\n<!-- \nThe following section should be included at the end of each chapter that contains code.\nNote that this will include code from code chunks even if `eval` is set to `false`.\n-->\n\n## @sec-simple Code\n\nThe following code was used to create @sec-simple.\n\n### Code within chapter\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load knitr package\nlibrary(knitr)\n\n# Python Engine Setup\nknit_engines$set(python3 = knit_engines$get(\"python\"))\n\n# Load packages\nlibrary(tidyverse)\nlibrary(gt)\n\n# Set default ggplot theme for document\ntheme_set(theme_classic())\n# If using kableExtra tables, print blank cells instead of `NA`\noptions(knitr.kable.NA = \"\")\n\n# Load NBA Data\nload(\"data/temp_wnba.RData\")\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta, expon, norm, gamma, binom\nimport os\n\n# Create directory if not exists\nfig_dir = \"fig\"\nos.makedirs(fig_dir, exist_ok=True)\n\n# Generate Data\nnp.random.seed(8990)\nn = 10000\nmean = [0, 0]\ncov = [[1, 0.8], [0.8, 1]]\nX = np.random.multivariate_normal(mean, cov, size=n)\nX1, X2 = X[:, 0], X[:, 1]\n\n# Transform U_X1 and U_X2 to uniform [0, 1] using the CDF of the normal distribution\nU_X1 = norm.cdf(X1)\nU_X2 = norm.cdf(X2)\n\n# Transform U_X1 and U_X2 into Gamma and Beta distributions\nY1 = gamma.ppf(U_X1, a=3, scale=1/15) \nY2 = beta.ppf(U_X2, a=5, b=3)\n\n# Calculate Pearson Correlation Coefficients\nrho_X = np.corrcoef(X1, X2)[0, 1]\nrho_Y = np.corrcoef(Y1, Y2)[0, 1]\nprint(\"Pearson correlation for (X1, X2):\", rho_X)\nprint(\"Pearson correlation for (Y1, Y2):\", rho_Y)\n\n# Create Layout design and Set Size-Ratio\nfig, axes = plt.subplots(1, 2, figsize=(6, 4))\n\n# Scatter plot for (X1, X2)\naxes[0].scatter(X1, X2, alpha=0.3, s=5)\naxes[0].set_title(\"Scatter plot of (X1, X2)\")\naxes[0].set_xlabel(\"X1\")\naxes[0].set_ylabel(\"X2\")\n\n# Add marginal histograms\naxes[0].hist(X1, bins=50, density=True, alpha=0.9, color='blue', orientation='vertical', histtype='step')\naxes[0].hist(X2, bins=50, density=True, alpha=0.9, color='red', histtype='step', orientation='horizontal')\n\n# Scatter plot for (Y1, Y2)\naxes[1].scatter(Y1, Y2, alpha=0.3, s=5)\naxes[1].set_title(\"Scatter plot of (Y1, Y2)\")\naxes[1].set_xlabel(\"Y1\")\naxes[1].set_ylabel(\"Y2\")\n\n# Add marginal histograms\naxes[1].hist(Y1, bins=50, density=True, alpha=0.9, color='blue', orientation='vertical', histtype='step')\naxes[1].hist(Y2, bins=50, density=True, alpha=0.9, color='red', histtype='step', orientation='horizontal')\n\n# Organize into a tight layout as per matplotlib\nplt.tight_layout()\n\n# Save figure instead of showing it\nfig_path = os.path.join(fig_dir, \"motivating_example.png\")\nplt.savefig(fig_path, dpi=300, bbox_inches='tight')\n\n# Close the figure to prevent rendering output\nplt.close(fig)  \nknitr::include_graphics(\"fig/motivating_example.png\")\n\n# Set random seed for reproducibility\nnp.random.seed(8990)\n\n# Apply probability integral transformation to all variables to make them uniform\nU_Y1 = gamma.cdf(Y1, a=3, scale=1/15)\nU_Y2 = beta.cdf(Y2, a=5, b=3)\n\n# Calculate Pearson Correlation Coefficients\nrho_U_X = np.corrcoef(U_X1, U_X2)[0, 1]\nrho_U_Y = np.corrcoef(U_Y1, U_Y2)[0, 1]\nprint(\"Pearson correlation for ($F_1(X_1)$, $F_2(X_2)$):\", rho_U_X)\nprint(\"Pearson correlation for ($G_1(Y_1)$, $G_2(Y_2)$):\", rho_U_Y)\n\n# Combine transformed data\nuniform_data = np.vstack([U_X1, U_X2, U_Y1, U_Y2]).T\n\n# Verify the uniformity of transformed data (Should be 0.5 in value)\nprint(\"U_X1 mean:\", U_X1.mean(), \"U_X2 mean:\", U_X2.mean())\nprint(\"U_Y1 mean:\", U_Y1.mean(), \"U_Y2 mean:\", U_Y2.mean())\n\n# Create Layout design and Set Size-Ratio\nfig, axes = plt.subplots(1, 2, figsize=(6, 4))\n\n# Scatter plot for (U_X1, U_X2)\naxes[0].scatter(U_X1, U_X2, alpha=0.3, s=5)\naxes[0].set_title(\"Scatter plot of ($F_1(X_1)$, $F_2(X_2)$)\")\naxes[0].set_xlabel(\"$F_1(X_1)$\")\naxes[0].set_ylabel(\"$F_2(X_2)$\")\n\n# Add marginal histograms\naxes[0].hist(U_X1, bins=50, density=True, alpha=0.9, color='blue', orientation='vertical', histtype='step')\naxes[0].hist(U_X2, bins=50, density=True, alpha=0.9, color='red', histtype='step', orientation='horizontal')\n\n# Scatter plot for (U_Y1, U_Y2)\naxes[1].scatter(U_Y1, U_Y2, alpha=0.3, s=5)\naxes[1].set_title(\"Scatter plot of ($G_1(Y_1)$, $G_2(Y_2)$)\")\naxes[1].set_xlabel(\"$G_1(Y_1)$\")\naxes[1].set_ylabel(\"$G_2(Y_2)$\")\n\n# Add marginal histograms\naxes[1].hist(U_Y1, bins=50, density=True, alpha=0.9, color='blue', orientation='vertical', histtype='step')\naxes[1].hist(U_Y2, bins=50, density=True, alpha=0.9, color='red', histtype='step', orientation='horizontal')\n\n# Organize into a tight layout as per matplotlib\nplt.tight_layout()\n\n# Save figure instead of showing it\nfig_path = os.path.join(fig_dir, \"transformed_motivating_example.png\")\nplt.savefig(fig_path, dpi=300, bbox_inches='tight')\n\n# Close the figure to prevent rendering output\nplt.close(fig)  \nknitr::include_graphics(\"fig/transformed_motivating_example.png\")\n\n# Set random seed for reproducibility\nnp.random.seed(8990)\n\n# Transform (Y1, Y2) back to normal marginals using quantile transformation\nF1_Y1 = norm.ppf(gamma.cdf(Y1, a=3, scale=1/15))\nF2_Y2 = norm.ppf(beta.cdf(Y2, a=5, b=3))\n\n# Calculate Pearson Correlation Coefficients\nrho_F_Y = np.corrcoef(F1_Y1, F2_Y2)[0, 1]\nprint(\"Pearson correlation for transformed:\", rho_F_Y)\nprint(\"Pearson correlation between X1 and X2:\", rho_X)\n\n# Plot the scatter plots with marginal histograms\nfig, axes = plt.subplots(1, 2, figsize=(6, 4))\n\n# Scatter plot for original normal marginals (X1, X2)\naxes[0].scatter(X1, X2, alpha=0.3, s=10)\naxes[0].set_title(\"Scatter plot ($F_1(X_1)$, $F_2(X_2)$)\")\naxes[0].set_xlabel(\"$F_1(X_1)$\")\naxes[0].set_ylabel(\"$F_2(X_2)$\")\naxes[0].hist(X1, bins=50, density=True, alpha=0.6, color='blue', histtype='step')\naxes[0].hist(X2, bins=50, density=True, alpha=0.6, color='red', histtype='step', orientation='horizontal')\n\n# Scatter plot for transformed normal marginals (F1_Y1, F2_Y2)\naxes[1].scatter(F1_Y1, F2_Y2, alpha=0.3, s=10)\naxes[1].set_title(\"Scatter plot ($F_1^{-1}(G_1(Y_1))$, $F_2^{-1}(G_2(Y_2))$)\")\naxes[1].set_xlabel(\"$F_1^{-1}(G_1(Y_1))$\")\naxes[1].set_ylabel(\"$F_2^{-1}(G_2(Y_2))$\")\naxes[1].hist(F1_Y1, bins=50, density=True, alpha=0.6, color='blue', histtype='step')\naxes[1].hist(F2_Y2, bins=50, density=True, alpha=0.6, color='red', histtype='step', orientation='horizontal')\n\n# Layout adjustment and save the figure\nplt.tight_layout()\nfig_path = os.path.join(fig_dir, \"quantile_transformed_motivating_example.png\")\nplt.savefig(fig_path, dpi=300, bbox_inches=\"tight\")\n\n# Close the figure to prevent rendering output\nplt.close(fig)  \n\nknitr::include_graphics(\"fig/quantile_transformed_motivating_example.png\")\n\n# =======================================================================\n# Sample R script for thesis template\n#\n# Cleans temp_raw_wnba.csv dataset, which contains data pulled from\n# https://www.espn.com/wnba/stats/player on 2024/06/19\n#\n# Last updated: 2024/06/19\n# =======================================================================\nlibrary(tidyverse)\n\nwnba <- read_csv(\"data/temp_raw_wnba.csv\") |> \n  janitor::clean_names() |> \n  # Pull jersey numbers off of names and \n  # turn height text into msmt (6'4\" = 6.3333)\n  mutate(jersey = str_extract(name, \"[0-9]+$\"),\n         name = str_remove(name, \"[0-9]+$\"),\n         ht_ft = parse_number(str_extract(ht, \"^[0-9]\")),\n         ht_in = parse_number(str_extract(ht, '[0-9]+\\\\\"$')),\n         height = ht_ft * 12 + ht_in,\n         weight = parse_number(wt),\n         position = factor(pos,\n                           levels = c(\"G\", \"F\", \"C\"),\n                           labels = c(\"Guard\", \"Forward\", \"Center\"))) |> \n  select(-c(ht, wt, ht_ft, ht_in, pos))\n  \nsave(wnba, file = \"data/temp_wnba.RData\")\n```\n:::\n\n\n### Code sourced from external scripts\n\n::: {.cell}\n\n```{.r .cell-code}\n# =======================================================================\n# Sample R script for thesis template\n#\n# Cleans temp_raw_wnba.csv dataset, which contains data pulled from\n# https://www.espn.com/wnba/stats/player on 2024/06/19\n#\n# Last updated: 2024/06/19\n# =======================================================================\nlibrary(tidyverse)\n\nwnba <- read_csv(\"data/temp_raw_wnba.csv\") |> \n  janitor::clean_names() |> \n  # Pull jersey numbers off of names and \n  # turn height text into msmt (6'4\" = 6.3333)\n  mutate(jersey = str_extract(name, \"[0-9]+$\"),\n         name = str_remove(name, \"[0-9]+$\"),\n         ht_ft = parse_number(str_extract(ht, \"^[0-9]\")),\n         ht_in = parse_number(str_extract(ht, '[0-9]+\\\\\"$')),\n         height = ht_ft * 12 + ht_in,\n         weight = parse_number(wt),\n         position = factor(pos,\n                           levels = c(\"G\", \"F\", \"C\"),\n                           labels = c(\"Guard\", \"Forward\", \"Center\"))) |> \n  select(-c(ht, wt, ht_ft, ht_in, pos))\n  \nsave(wnba, file = \"data/temp_wnba.RData\")\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}