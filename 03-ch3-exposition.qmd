# Copulas and Association Measures {#sec-exposition}

```{r}
#| label: setup
#| include: false

# Load knitr package
library(knitr)

# Python Engine Setup
knit_engines$set(python3 = knit_engines$get("python"))

# Load packages
library(tidyverse)
library(gt)

# Set default ggplot theme for document
theme_set(theme_classic())
# If using kableExtra tables, print blank cells instead of `NA`
options(knitr.kable.NA = "")

# Load NBA Data
load("data/temp_wnba.RData")
```

## Unraveling the Notion of Dependence

In this section, we aim to formalize concepts of dependence and association. To facilitate our understanding, we will use two bivariate random vectors and visualize their relationships through Python code.

### Motivating Example

Consider $(X_1, X_2)$ and $(Y_1, Y_2)$ be bivariate random vectors, each consisting of 10000 independent data-points, which are distributed with the joint distributions $F_X$ and $F_Y$ respectively. Given these bivariate vectors, one might ask: How can I compare the relationship between $(X_1,X_2)$ to the relationship between $(Y_1, Y_2)$? One of the measures that can help us compare and contrast these relationships is Pearson correlation coefficient (commonly denoted as $\rho_{pearson}$). After preliminary calculations on a Python3 kernel, we can see that $\rho_{pearson}(X_1, X_2) \approx 0.802$, but on the other hand, the correlation between $\rho_{pearson}(Y_1, Y_2) \approx 0.755$. From these measure-values, it seems that the dependence between $(X_1,X_2)$ is stronger than the dependence between $(Y_1, Y_2)$. Although this agrees with our scatter plots in @fig-motivating-example, it is vital to note that $\rho_{pearson}$ only captures the linear dependence between the underlying random variables at hand.

Upon observing the @fig-motivating-example closely, we note that the marginal distributions of $X_1$ and $X_2$ are close to normal, unlike the marginals of $Y_1$ and $Y_2$. Moreover, we can see that the relationship between $Y_1$ and $Y_2$ is non-linear. This vast difference in marginals takes away our trust from the appropriateness of the use of $\rho_{pearson}$ as a measure to compare dependence between the data vectors at hand.

```{python3}
#| label: gen-data-motivating-example
#| echo: false
#| output: false

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import beta, expon, norm, gamma, binom
import os

# Create directory if not exists
fig_dir = "fig"
os.makedirs(fig_dir, exist_ok=True)

# Generate Data
np.random.seed(8990)
n = 10000
mean = [0, 0]
cov = [[1, 0.8], [0.8, 1]]
X = np.random.multivariate_normal(mean, cov, size=n)
X1, X2 = X[:, 0], X[:, 1]

# Transform U_X1 and U_X2 to uniform [0, 1] using the CDF of the normal distribution
U_X1 = norm.cdf(X1)
U_X2 = norm.cdf(X2)

# Transform U_X1 and U_X2 into Gamma and Beta distributions
Y1 = gamma.ppf(U_X1, a=3, scale=1/15) 
Y2 = beta.ppf(U_X2, a=5, b=3)

# Calculate Pearson Correlation Coefficients
rho_X = np.corrcoef(X1, X2)[0, 1]
rho_Y = np.corrcoef(Y1, Y2)[0, 1]
print("Pearson correlation for (X1, X2):", rho_X)
print("Pearson correlation for (Y1, Y2):", rho_Y)

# Create Layout design and Set Size-Ratio
fig, axes = plt.subplots(1, 2, figsize=(6, 4))

# Scatter plot for (X1, X2)
axes[0].scatter(X1, X2, alpha=0.3, s=5)
axes[0].set_title("Scatter plot of (X1, X2)")
axes[0].set_xlabel("X1")
axes[0].set_ylabel("X2")

# Add marginal histograms
axes[0].hist(X1, bins=50, density=True, alpha=0.9, color='blue', orientation='vertical', histtype='step')
axes[0].hist(X2, bins=50, density=True, alpha=0.9, color='red', histtype='step', orientation='horizontal')

# Scatter plot for (Y1, Y2)
axes[1].scatter(Y1, Y2, alpha=0.3, s=5)
axes[1].set_title("Scatter plot of (Y1, Y2)")
axes[1].set_xlabel("Y1")
axes[1].set_ylabel("Y2")

# Add marginal histograms
axes[1].hist(Y1, bins=50, density=True, alpha=0.9, color='blue', orientation='vertical', histtype='step')
axes[1].hist(Y2, bins=50, density=True, alpha=0.9, color='red', histtype='step', orientation='horizontal')

# Organize into a tight layout as per matplotlib
plt.tight_layout()

# Save figure instead of showing it
fig_path = os.path.join(fig_dir, "motivating_example.png")
plt.savefig(fig_path, dpi=300, bbox_inches='tight')

# Close the figure to prevent rendering output
plt.close(fig)  
```

```{r}
#| label: fig-motivating-example
#| fig-cap: "Scatter plots of 10000 independent observations of $(X_1, X_2)$ and $(Y_1, Y_2)$ with overlaid curves depicting respective marginal distributions."

knitr::include_graphics("fig/motivating_example.png")

```

Let's introduce a lemma that will help us transform the marginals so that the resulting marginals are more similar, and try to only capture or extract the "dependence" components, which will allow us to make fairer comparisons. 

::: {#lem-prob-int-trans}
### Probability Integral Transformation
[@ecmr]
Let $F$ be a continuous distribution function and let $X \sim F$, then $F(X)$ is a standard uniform random variable, that is, $F(X) \sim U(0,1)$.
:::

@lem-prob-int-trans allows us to transform a continuous random variable to a random variable which has standard uniform distribution. So, by using this transformation, we can now convert our marginals $X_1, X_2, Y_1, Y_2$ individually to be distributed $\text{Uniform}(0,1)$. And, since now the resulting marginals will all be of the same type, it will allow us to compare the dependence between random variables on fairer grounds.

For instance, if we know that $X_1 \sim N(0,1) = F_1$, $X_2 \sim N(0,1) = F_2$, $Y_1 \sim Gamma(3, 15) = G_1$, and $Y_2 \sim Beta(5, 3) = G_2$, where $F_1, F_2, G_1, G_2$ denote the distribution functions of the respective random variables. By @lem-prob-int-trans, we can say that $F_1(X_1), F_2(X_2), G_1(Y_1),$ and $G_2(Y_2)$ are each distributed $\text{Uniform}(0,1)$.

```{python3}
#| label: gen-data-transformed-motivating-example
#| echo: false
#| output: false

# Set random seed for reproducibility
np.random.seed(8990)

# Apply probability integral transformation to all variables to make them uniform
U_Y1 = gamma.cdf(Y1, a=3, scale=1/15)
U_Y2 = beta.cdf(Y2, a=5, b=3)

# Calculate Pearson Correlation Coefficients
rho_U_X = np.corrcoef(U_X1, U_X2)[0, 1]
rho_U_Y = np.corrcoef(U_Y1, U_Y2)[0, 1]
print("Pearson correlation for ($F_1(X_1)$, $F_2(X_2)$):", rho_U_X)
print("Pearson correlation for ($G_1(Y_1)$, $G_2(Y_2)$):", rho_U_Y)

# Combine transformed data
uniform_data = np.vstack([U_X1, U_X2, U_Y1, U_Y2]).T

# Verify the uniformity of transformed data (Should be 0.5 in value)
print("U_X1 mean:", U_X1.mean(), "U_X2 mean:", U_X2.mean())
print("U_Y1 mean:", U_Y1.mean(), "U_Y2 mean:", U_Y2.mean())

# Create Layout design and Set Size-Ratio
fig, axes = plt.subplots(1, 2, figsize=(6, 4))

# Scatter plot for (U_X1, U_X2)
axes[0].scatter(U_X1, U_X2, alpha=0.3, s=5)
axes[0].set_title("Scatter plot of ($F_1(X_1)$, $F_2(X_2)$)")
axes[0].set_xlabel("$F_1(X_1)$")
axes[0].set_ylabel("$F_2(X_2)$")

# Add marginal histograms
axes[0].hist(U_X1, bins=50, density=True, alpha=0.9, color='blue', orientation='vertical', histtype='step')
axes[0].hist(U_X2, bins=50, density=True, alpha=0.9, color='red', histtype='step', orientation='horizontal')

# Scatter plot for (U_Y1, U_Y2)
axes[1].scatter(U_Y1, U_Y2, alpha=0.3, s=5)
axes[1].set_title("Scatter plot of ($G_1(Y_1)$, $G_2(Y_2)$)")
axes[1].set_xlabel("$G_1(Y_1)$")
axes[1].set_ylabel("$G_2(Y_2)$")

# Add marginal histograms
axes[1].hist(U_Y1, bins=50, density=True, alpha=0.9, color='blue', orientation='vertical', histtype='step')
axes[1].hist(U_Y2, bins=50, density=True, alpha=0.9, color='red', histtype='step', orientation='horizontal')

# Organize into a tight layout as per matplotlib
plt.tight_layout()

# Save figure instead of showing it
fig_path = os.path.join(fig_dir, "transformed_motivating_example.png")
plt.savefig(fig_path, dpi=300, bbox_inches='tight')

# Close the figure to prevent rendering output
plt.close(fig)  
```

```{r}
#| label: fig-transformed-motivating-example
#| fig-cap: "Scatter plots of 10000 independent observations of ($F_1(X_1)$, $F_2(X_2)$) and ($G_1(Y_1)$, $G_2(Y_2)$) with overlaid curves depicting respective marginal distributions."

knitr::include_graphics("fig/transformed_motivating_example.png")

```

Looking at @fig-transformed-motivating-example, we can see that the transformed data vectors appear to be significantly similar. We can computationally verify this by quickly calculating the $\rho_{pearson}$ for ($F_1(X_1)$, $F_2(X_2)$) and ($G_1(Y_1)$, $G_2(Y_2)$), which turns out to be 0.788 for both data vector pairs, meaning that both have same dependence structures.

An alternative way to approach the problem (of comparing dependence of distinct pairs of marginals), is by transforming the marginals of $(Y_1, Y_2)$ to be normal (same as marginals of $(X_1, X_2)$). As one can predict, in order to accomplish this transformation, we would need to "undo the current distributional mappings on $(Y_1, Y_2)$", which we can formally define as generalized inverse as follows:

::: {#def-quantile-function}
### Quantile Function
[@ecmr]
$F^{\leftarrow}$ (Quantile Function) is defined as $F^{\leftarrow}(y) = \text{inf} \{x \in \mathbb{R} | F(x) \geq y \}$, where $y \in [0, 1]$, and $\text{inf}$ is the infimum of a set.
:::

:::{.callout-warning}
The quantile function $F^{\leftarrow} = F^{-1}$ only when $F$ is continuous and strictly increasing. Thus it is important to note that, in other cases, the ordinary inverse $F^{-1}$ need not exist. [@ecmr]
:::

With the above definition of $F^{\leftarrow}$, let's introduce a lemma from [@ecmr] that will help us perform the transformation to normal.

::: {#lem-quantile-transformation}
### Quantile Transformation
[@ecmr]
Let $U \sim \text{Unif}(0, 1)$ and let $F$ be any distribution function be a distribution function. Then $F^{\leftarrow}(U) \sim F$, that is, $F^{\leftarrow}(X)$ is distributed with density $F$.
:::

:::{.callout-note}
@lem-quantile-transformation is valid for non-continuous densities $F$ as well. [@ecmr]
:::

Let's start with the transformations where we left off in @fig-transformed-motivating-example, since we have uniform densities there. Applying @lem-quantile-transformation on $G_1(Y_1)$ and $G_2(Y_2)$ using quantile functions $F_1^{\leftarrow} = F_1^{-1}$ and $F_2^{\leftarrow} = F_2^{-1}$ respectively gives us that $F_1^{-1}(G_1(Y_1)) \sim F_1$ and $F_2^{-1}(G_2(Y_2)) \sim F_2$. 

```{python3}
#| label: gen-data-quantile-transformed-motivating-example
#| echo: false
#| output: false

# Set random seed for reproducibility
np.random.seed(8990)

# Transform (Y1, Y2) back to normal marginals using quantile transformation
F1_Y1 = norm.ppf(gamma.cdf(Y1, a=3, scale=1/15))
F2_Y2 = norm.ppf(beta.cdf(Y2, a=5, b=3))

# Calculate Pearson Correlation Coefficients
rho_F_Y = np.corrcoef(F1_Y1, F2_Y2)[0, 1]
print("Pearson correlation for transformed:", rho_F_Y)
print("Pearson correlation between X1 and X2:", rho_X)

# Plot the scatter plots with marginal histograms
fig, axes = plt.subplots(1, 2, figsize=(6, 4))

# Scatter plot for original normal marginals (X1, X2)
axes[0].scatter(X1, X2, alpha=0.3, s=10)
axes[0].set_title("Scatter plot ($F_1(X_1)$, $F_2(X_2)$)")
axes[0].set_xlabel("$F_1(X_1)$")
axes[0].set_ylabel("$F_2(X_2)$")
axes[0].hist(X1, bins=50, density=True, alpha=0.6, color='blue', histtype='step')
axes[0].hist(X2, bins=50, density=True, alpha=0.6, color='red', histtype='step', orientation='horizontal')

# Scatter plot for transformed normal marginals (F1_Y1, F2_Y2)
axes[1].scatter(F1_Y1, F2_Y2, alpha=0.3, s=10)
axes[1].set_title("Scatter plot ($F_1^{-1}(G_1(Y_1))$, $F_2^{-1}(G_2(Y_2))$)")
axes[1].set_xlabel("$F_1^{-1}(G_1(Y_1))$")
axes[1].set_ylabel("$F_2^{-1}(G_2(Y_2))$")
axes[1].hist(F1_Y1, bins=50, density=True, alpha=0.6, color='blue', histtype='step')
axes[1].hist(F2_Y2, bins=50, density=True, alpha=0.6, color='red', histtype='step', orientation='horizontal')

# Layout adjustment and save the figure
plt.tight_layout()
fig_path = os.path.join(fig_dir, "quantile_transformed_motivating_example.png")
plt.savefig(fig_path, dpi=300, bbox_inches="tight")

# Close the figure to prevent rendering output
plt.close(fig)  

```

```{r}
#| label: fig-quantile-transformed-motivating-example
#| fig-cap: "Scatter plots of 10000 independent observations of ($X_1$, $X_2$) and ($F_1^{-1}(G_1(Y_1))$, $F_2^{-1}(G_2(Y_2))$) with overlaid curves depicting respective marginal distributions."

knitr::include_graphics("fig/quantile_transformed_motivating_example.png")

```

Notice in @fig-quantile-transformed-motivating-example that the resulting transformed distribution through this alternative method resembles that of $(X_1, X_2)$. Hence, we can conclude that they have the same dependence. Furthermore, through a quick calculation, we can see that $\rho_{pearson}(F_1^{-1}(G_1(Y_1)), F_2^{-1}(G_2(Y_2))) = 0.802$, which is the same as the Pearson correlation coefficient between $X_1$ and $X_2$. This is the level of flexibility that a combination of transformations presented in @lem-prob-int-trans and @lem-quantile-transformation can lend us.

:::{.callout-note}
"$(X_1, X_2)$ and $(Y_1, Y_2)$ have the same dependence" $\iff$ "$(X_1, X_2)$ and $(Y_1, Y_2)$ have the same copula" [@ecmr]
:::

## Copulas as a Unified Framework for Dependence

Copulas are a class of multivariate distribution functions with $Unif(0, 1)$ marginals. The motivating example in the previous section explains the usage of copulas as the structures capturing margin-independent dependence between random variables.

:::{.callout-note}
The choice of $Unif(0, 1)$ as a post-transformation margin for the data at hand is  somewhat arbitrary although it does simplify further results. One can use modifications of @lem-prob-int-trans and @lem-quantile-transformation to define copulas with respect to any margin of choice without affecting the final conclusions about the dependence between the data at hand. [@ecmr]
:::

In order to understand copulas better, for now, let's restrict ourselves to the 2-D (2-dimensional) case. Firstly, let's introduce the definition of a broader class of functions called subcopulas as a preliminary, which will help us mathematically define copulas as a special case. [@nelsen]

::: {#def-2d-subcopula}
### 2-Dimensional Subcopula
[@Erdely2017]
A **two-dimensional subcopula** (2-subcopula) is a function $C^S: D_1 \times D_2 \to [0, 1]$, where $\{0, 1\} \subseteq D_i \subseteq [0, 1]$ for $i \in \{1, 2\}$ with the following conditions satisfied:

* _Grounded:_ $C^S(u, 0) = 0 = C^S(0, v)$, $\forall u \in D_1, \forall v \in D_2$.

* _Marginal Consistency:_ $\forall u \in D_1$ and $\forall v \in D_2$, $C^S(u, 1) = u$ and $C^S(1, v) = v$.

* _2-increasing:_ $\forall u_1,u_2 \in D_1$ and $\forall v_1, v_2 \in D_2$ such that $u_1 \leq u_2$ and $v_1 \leq v_2$, $C^S(u_1, v_1) - C^S(u_2, v_1) + C^S(u_2, v_2) - C^S(u_1, v_2) \geq 0$.
:::

::: {#def-2d-copula}
### 2-Dimensional Copula
[@Erdely2017]
A **two-dimensional copula** (2-copula) is a function $C: [0, 1] \times [0, 1] \to [0, 1]$, with the following conditions satisfied:

* _Grounded:_ $C(u, 0) = 0 = C(0, v)$, $\forall u \in [0, 1], \forall v \in [0, 1]$.

* _Marginal Consistency:_ $\forall u \in [0, 1]$ and $\forall v \in [0, 1]$, $C(u, 1) = u$ and $C(1, v) = v$.

* _2-increasing:_ $\forall u_1,u_2 \in [0, 1]$ and $\forall v_1, v_2 \in [0, 1]$ such that $u_1 \leq u_2$ and $v_1 \leq v_2$, $C(u_1, v_1) - C(u_2, v_1) + C(u_2, v_2) - C(u_1, v_2) \geq 0$.
:::

:::{.callout-note}
A 2-D copula is essentially a 2-subcopula with a full unit square as domain ($D_1 = D_2 = [0, 1]$). Furthermore, copula and subcopula are the same within a domain with continuous variables. Later in this chapter, we will discuss why this doesn't hold when one of the variables is discrete. 
:::

In this work, we will mainly deal with 2-D copulas and subcopulas, but the definitions above can be generalized to n-D case with some notable exceptions detailed (with proofs) in section 2.10 of @nelsen. Moreover, there are many different families of copulas bearing peculiar properties and corresponding margins, we are not covering them in detail since that is not the focus of this work, and a comprehensive summary of many of these families can be found in chapter 3 of @ecmr.

### Fréchet-Hoeffding Bounds

For any distribution function, boundedness is always a desired property. In the case of copulas, we have a famous theorem that provides us the upper and lower pointwise bounds.

::: {#thm-fh-bounds}
### Fréchet-Hoeffding Bounds
[@ecmr]
Given a 2-D copula $C$, $W(u,v) = \max\{0, u+v-1\} \leq C(u,v) \leq \min\{u,v\} = M(u,v)$, where $u, v \in [0,1]$.
:::

## Sklar's Theorem and Invariance Principle

@thm-sklar by [@sklar1959fonctions] is one of the seminal results in copula theory, which extended the applications of copulas, and explained why copulas captures the dependence by relating the joint distributions to univariate margins.

::: {#thm-sklar}
### Fréchet-Hoeffding Bounds
[@ecmr]

1. Let $H$ be a joint distribution function with univariate margins $F$ and $G$. Then there exists a copula $C$ such that $\forall x,y \in \mathbb{R}, H(x,y) = C(F(x), G(y))$. Furthermore, $C$ is **unique** in the case when $F, G$ are continuous; otherwise, in the general case, $C$ is uniquely determined on $\text{Ran}F \times \text{Ran}G$, where $\text{Ran}F, \text{Ran}G$ denote the ranges of $F, G$ respectively. That copula $C$ is given by: $C(u,v)=H(F^{\leftarrow}(u), G^{\leftarrow}(u))$ such that $(u,v) \in \text{Ran}F \times \text{Ran}G$.

2. Conversely, $H$ is defined as a 2-D distribution function with marginals $F, G$, if we are given copula $C$ along with the univariate marginals $F, G$.
:::

In this work, we will mainly deal with two dimensions, but @thm-sklar above can be generalized to n-D case as detailed in section 2.10 of @nelsen. Below, we include a few insights drawn from [@ecmr] that will be important to our ongoing discussion:

:::{.callout-note}
@thm-sklar gives us an insight into the name copula as in how it "couples" a joint distribution function to its marginal distributions. This coupling effect and two parts of @thm-sklar show us how we can separate (or combine) multivariate dependence structure and univariate margins.
:::

:::{.callout-warning}
### Spoiler Alert
In the case of continuous random variables, there is only one **unique** copula that characterizes the multivariate dependence structure, which is very convenient for reasons we will discuss later in this chapter. This is not the case with discrete variables, which make the direct use of continuous copulas intractable.
:::

:::{.callout-note}
@thm-sklar can be used to verify the existence of a continuous distribution function $H$ in case of a multivariate dataset if and only if we are sure of the existence of corresponding continuous univariate marginals for each variable in the dataset.
:::

### The Invariance Principle

As we saw in the motivating example, the underlying dependence structure did not change over a certain type of transformations. This was very convenient for us, and thus is a favorable property for a copula to have. This property is often formally referred to as "invariance", which we will formalize in the following theorem from [@ecmr]

::: {#thm-invariance}
### Invariance Principle
Let $(X, Y) \sim H$ with continuous margins $F, G$ and copula $C$. If $T_X, T_Y$ are **strictly increasing** transformations on $\text{Ran}X, \text{Ran}Y$, respectively, then $(T_X(X), T_Y(Y))$ also has copula $C$.
:::

:::{.callout-note}
@thm-invariance was implicitly in action during our analysis for the motivating example because the transformations that we used were of two kinds, namely, probability integral transformation and quantile transformation, and in both of the cases, we were dealing with continuous and **strictly increasing** mappings on the respective ranges of random variables.
:::

## Copulas for Continuous and Discrete Data

-  you can  discuss Challenges and Pitfalls When Applying Continuous Copulas to Discrete Data

- in discrete case, copula is not as flexible as in continous case

Up to this point, our discussion has centered on continuous random variables. Many of the results and definitions we have used rely on continuity, which ensures that the probability integral transform (PIT) maps each variable to a uniform distribution on $[0,1]$. This property, in turn, guarantees the uniqueness of the copula associated with a joint distribution via Sklar’s theorem. In our earlier work, we have taken this uniqueness for granted.

However, real-world data are often **discrete**. When dealing with discrete random variables, the marginal distribution functions are not continuous, and the PIT no longer produces uniform random variables on the full interval $[0,1]$. Instead, we obtain what is known as a **subcopula**—a function defined only on a proper subset of $[0,1]^2$, namely on the ranges of the marginal distributions.

::: {.callout-note}
### Example: Bivariate Bernoulli Distribution

*Imagine a bivariate distribution where each variable follows a Bernoulli law. In this setting, the only possible values for each variable are 0 and 1. The resulting subcopula is then defined on the set of points. Because this set is a proper subset of $[0,1]^2$, the corresponding copula is not uniquely determined by the joint distribution of the variables.*

:::

### Unidentifiability Issue

Now, let us examine the unidentifiability problem in more detail. To illustrate the issue, consider the following adapted example in the two-dimensional case, inspired by @Geenens. Suppose we have a subcopula $C^S$ defined on a discrete domain, where $D_1=\operatorname{Ran}(F)$ and $D_2=\operatorname{Ran}(G)$ with the marginal distribution functions $F$ and $G$, respectively. In the continuous case, a two-dimensional (sub)copula is defined on the entire unit square $[0,1]^2$. By contrast, for discrete random variables, the subcopula $C^S$ is only uniquely specified on the domain $D_1 \times D_2 = \operatorname{Ran}(F) \times \operatorname{Ran}(G)$.

To obtain a full copula $C$ on $[0,1]^2$, one must “fill in” the gaps—that is, extend the definition of $C^S$ to those parts of the unit square not covered by $D_1 \times D_2$. Unfortunately, there are uncountably many ways to perform this extension while still satisfying the fundamental properties required of a copula in its @def-2d-copula. This leads to a **non-uniqueness** (or **unidentifiability**) issue, which complicates both the development and the application of copula-based models for discrete data. This unidentifiability has been examined in depth in the literature such as @Geenens, and it calls into question the straightforward (direct) application of copula methods when at least one margin is discrete.

One of the ways to fill in the gaps is by performaing a Distributional Transform, which basically serves to add random "noise" to each of the gaps in parent distribution as described by @RUSCHENDORF and @Faugeras. Formally, considering a random variable $X \sim F$ and independently, consider $V \sim U(0,1)$, then the distributional transform of $X$ is $F(X, V) = P(X < x) + V * P(X = x)$. After applying this, we can directly proceed to apply results from continuous copula modeling as we have smoothened out the discontinuities. Another method that also accomplishes this goal is described in the next chapter.

-  here, you can introduce Checkerboard Copula  


## Copula-based Measures of Association and Estimation

Now that we have built an object (copula) that allows us to just capture the multivariate dependence structure between variables, we would like to encode certain pieces of this information into a set of robust measures or metrics. We would call these measures, the **measures of association**. There are two types of measures of association: parametric and non-parametric. As discussed briefly for our motivating example, a common (parametric) measure of association is the Pearson correlation coefficient ($\rho_{pearson}$). Although it is really efficient to calculate, it only captures linear dependence between the random data vectors at hand. Let's discuss this metric in more detail along with its limitations:

### Pearson's Correlation Coefficient ($\rho_{pearson}$) & its Properties

::: {#def-pearson}
### Pearson correlation coefficient
Given a random vector $(X, Y)$ with $Var(X) < \infty$ and $Var(Y) < \infty$, then:

$$
\rho_{pearson}(X, Y) = \frac{Cov(X, Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}}
$$
, where covariance is defined as: 
$$
Cov(X, Y) = \mathbb{E}((X - \mathbb{E}(X))(Y - \mathbb{E}(Y)))
$$
, and the variance is defined as $Var(X) = \mathbb{E}((X - \mathbb{E}(X))^2)$.
:::

Let's start by going over some commonly-used properties of $\rho_{pearson}$ as mentioned in @ecmr:

1. $\rho_{pearson} \in [-1, 1]$

2. $|\rho_{pearson}(X, Y)|$ = 1 if and only if $\exists a,b \in \mathbb{R}$, with $a \neq 0$ such that $Y = aX + b$ almost surely with $a < 0$ if and only if $\rho_{pearson}(X, Y) = -1$, and $a > 0$ if and only if $\rho_{pearson}(X, Y) = 1$. In both cases, $X, Y$ are called _perfectly linearly dependent_

3. If $X$ and $Y$ ar independent, then $\rho_{pearson}(X, Y) = 0$.

4. $\rho_{pearson}$ is invariant under _strictly increasing linear_ transformations.

### Limitations of Pearson's Correlation Coefficient ($\rho_{pearson}$)

Although Pearson’s correlation coefficient $\rho_{\text{pearson}}$ is useful in many cases, it only captures **linear dependence** and ignores non-linear relationships. Below, we summarize its key limitations along with illustrative examples.

1. **Non-Existence of $\rho_{pearson}$:** Pearson's correlation does not exist for every random vector $(X, Y)$, particularly when variances (or other higher order moments) are undefined.

:::{.callout-note}
#### Example: Heavy-Tailed Distributions
Consider two independent random variables $X_1, X_2$ drawn from a **Pareto(3)** distribution with $F(x) = 1 - x^{-3}, \quad x \geq 1$. Define $X = X_1$, and $Y = X_1^2$. The covariance is given by $Cov(X, Y) = Cov(X_1, X_1^2) = \mathbb{E}(X_1^3) - \mathbb{E}(X_1)\mathbb{E}(X_1^2)$. For Pareto(3), it is well-known (and can be easily proven) that $\mathbb{E}(X_1^3)$ **does not exist** (as the integral diverges). Since Pearson's formula rely on this moment, $\rho_{pearson}(X, Y)$ **doesn't exist**. On the other hand, we can observe that $Y = X^2$ shows a **perfect functional dependence**, since $Y$ can be represented as a deterministic (quadratic) function of $X$.
:::

2. **Non-Invariance Under Non-Linear Transformations:** $\rho_{pearson}$ is not necessarily invariant under all strictly increasing transformations on $\text{Ran}X$ or $\text{Ran}Y$.

:::{.callout-note}
#### Example: Logarithmic Transformation on $U(0, 1)$
Let $X \sim U(0,1)$ and define $Y = \log(X)$. Pearson’s correlation is: $\rho_{\text{pearson}}(X, Y) = \frac{\text{Cov}(X, \log X)}{\sigma_X \sigma_Y}$. Even though$Y = \log(X)$ is a **strictly increasing function**, $\rho_{pearson}$ changes under this transformation. Thus, Pearson's correlation is **not invariant** under (non-linear) monotonic transformations such as $\log$ in certain situations.
:::

3. **Uncorrelatedness Does Not Imply Independence:** $\rho_{pearson} = 0$ does NOT necessarily imply that $(X, Y)$ are independent.

:::{.callout-note}
#### Example: Quadratic Transformation on $U(-1, 1)$
Let $X \sim U(-1,1)$ and define: $Y = X^2$. We can compute: $\mathbb{E}[X] = 0, \quad \mathbb{E}[Y] = \mathbb{E}[X^2] = \frac{1}{3}$. Now, consider the covariance: $\text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y] = \mathbb{E}[X^3] - (0)(\frac{1}{3})$. Since $\mathbb{E}[X^3] = 0$, we get $\text{Cov}(X, Y) = 0$. Thus, $\rho_{\text{pearson}}(X, Y) = 0$, but **$X$ and $Y$ are clearly dependent**, since knowing $X$ exactly determines $Y$. This example demonstrates that a zero Pearson correlation does **not** imply statistical independence.
:::

4. **Non-Uniqueness of the Joint Distribution Given Marginals and $\rho_{pearson}$:** The marginal distributions and the correlation coefficient do not uniquely determine the joint distribution. 

:::{.callout-note}
#### Example: Bivariate Normal and Mixture Distributions
Consider two bivariate distributions:

1. **Bivariate Normal Distribution**:  
   $$
   (X_1, X_2) \sim N \left( \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix} \right).
   $$

2. **Bivariate Mixture Distribution** (Same Marginals, Different Dependence):  
   $$
   X_1 \sim N(0,1), \quad X_2 = \begin{cases} 
   X_1, & \text{with probability } 0.75, \\
   -X_1, & \text{with probability } 0.25.
   \end{cases}
   $$

Both cases yield: $\rho_{pearson}(X_1, X_2) = 0.5$.

However, their **joint distributions are completely different**, meaning **$\rho_{pearson}$ does not uniquely determine dependence**.
:::

5. **Unattainability of Certain Correlations:** Given margins $F_1, F_2$, some $\rho_{pearson} \in [-1,1]$ values cannot be attained by choosing any possible copula for $(X_1, X_2)$. An example demonstrating this can be found in @ecmr p.46

In order to circumvent some of the limitations of pearson coefficient, we now consider rank-based correlation measures such as Spearman's Rho ($\rho_{spearman}$) and Kendall's Tau ($\tau_{kendall}$) as they only depend on the underlying copula $C$ at least in the case of continuous random variables. Again, we will discuss the pecularities of the discrete case later in this chapter.

These rank-based measures are also known as **measures of concordance**. [@ecmr] In order to better understand this, we would first need to define _concordance_. Consider two points in $\mathbb{R}^2$, $(x_1, y_1)$ and $(x_2, y_2)$. These points are defined as concordant if $(x_1 - x_2)(y_1 - y_2) > 0$ and discordant if $(x_1 - x_2)(y_1 - y_2) < 0$.

### Kendall's Tau

::: {#def-kendalls-tau}
### Kendall's Tau
Given a bivariate random vector $(X_1, X_2)$ with continuous marginals $F_1$ and $F_2$, let's define $(X_1', X_2')$ as an independent copy of $(X_1, X_2)$. Then the population version of Kendall's tau is defined by:

$$
\tau_{kendall}(X_1, X_2) = \mathbb{E}(\text{sign}((X_1 - X_1')(X_2 - X_2')))
$$
Here, $\text{sign}(x)$ is the sign-function defined in a piecewise manner as follows:
$$
\text{sign}(x) = \begin{cases} 
   -1, & \text{if } x < 0, \\
   0, & \text{if } x = 0, \\
   1, & \text{if } x > 0.
   \end{cases}
$$
:::

Using the above-mentioned notion of concordance, definition of an expected value, and @def-kendalls-tau, we can equivalently define Kendall's Tau as $\tau_{kendall} = (1)\mathbb{P}((X_1 - X_1')(X_2 - X_2') > 0) + (0)\mathbb{P}((X_1 - X_1')(X_2 - X_2') = 0) + (-1)\mathbb{P}((X_1 - X_1')(X_2 - X_2') < 0) = \mathbb{P}((X_1 - X_1')(X_2 - X_2') > 0) - \mathbb{P}((X_1 - X_1')(X_2 - X_2') < 0)$, since in the case of continuous distributions, probability at any given point is 0, specifically $\mathbb{P}((X_1 - X_1')(X_2 - X_2') = 0) = 0$. 

As mentioned in @ecmr p.53, we can represent $\tau_{kendall}$ in terms of an underlying copula $C$ as $\tau_{kendall}(C) = 4 \int_{[0, 1]^2}C(u,v)d(C(u,v)) - 1$.  

- write on how issues with this copula representation in discrete case

### Spearman's Rho 

::: {#def-spearman}
### Spearman's Rho
Given a bivariate random vector $(X_1, X_2)$ with continuous marginals $F_1$ and $F_2$, then the population version of Spearman's rho is defined by:

$$
\rho_{spearman}(X_1, X_2) = \rho_{pearson}(F_1(X_1), F_2(X_2))
$$
We can observe that the Spearman's rho is nothing but Pearson's correlation coefficient of the transformed variables obtained after performing the Probability Integral Transformation defined earlier in @lem-prob-int-trans.
:::

As mentioned in @ecmr p.53, we can represent $\rho_{spearman}$ in terms of an underlying copula $C$ as $\rho_{spearman}(C) = 12 \int_{[0, 1]^2}C(u,v)d((u,v)) - 3$.


:::{.callout-note}
### Note:
$\tau_{kendall}$ and $\rho_{spearman}$ both overcome the significant limitations of $\rho_{pearson}$ with the following properties as summarized in @ecmr:

* These measures always exist, and are invariance under all (not just linear) strictly increasing tranformations

* These measures attain all values in $[-1, 1]$, and they specifically attain -1 and 1 when the copula $C$ attains the Fréchet-Hoeffding bounds $W$ and $M$ as defined in @thm-fh-bounds

:::

- review commonly used measures of association and discuss their relation with copulas  

- continuous case measures are already model-free, margin free and all, but this is not the case in discrete situations (use this for transition)

### Checkerboard Copula-based Association Measures for 2-Dimensional Data

- you can find in the references [5,11,12,38] in my 2021 paper.


